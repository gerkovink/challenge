[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Schedule\n\n\n\n\n\n\nTimeslot\nActiviteit\n\n\n\n\n09:00 - 09:30\nInloop\n\n\n09:30 - 10:30\nDe anatomie van een antwoord\n\n\n10:30 - 10:45\nPauze\n\n\n10:45 - 11:15\nInteractieve verkenning van real-world data imputation\n\n\n11:15 - 12:30\nVan ‘dark’ data naar imputatie en verder naar data synthese\n\n\n12:30 - 13:30\nLunch\n\n\n13:30 - 14:30\nHands-on met mice in R\n\n\n14:30 - 14:45\nPauze\n\n\n14:45 - 15:45\nData synthese challenge\n\n\n15:45 - 16:00\nPauze\n\n\n16:00 - 16:30\nNabespreking en prijsuitreiking"
  },
  {
    "objectID": "practicals/real-world-data-problems.html",
    "href": "practicals/real-world-data-problems.html",
    "title": "real-world-data-problems",
    "section": "",
    "text": "This is not really a tutorial yet, just a placeholder."
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Lecture slides\nHieronder vind je de lecture slides voor de verschillende onderdelen van de cursus.\n\nLecture 1: The Anatomy of an Answer\nLecture 2: From ‘dark’ data to imputation and beyond to data synthesis"
  },
  {
    "objectID": "lectures/anatomy.html#disclaimer",
    "href": "lectures/anatomy.html#disclaimer",
    "title": "The Anatomy of an Answer",
    "section": "",
    "text": "I owe a debt of gratitude to many people as the thoughts and code in these slides are the process of years-long development cycles and discussions with my team, friends, colleagues and peers. When someone has contributed to the content of the slides, I have credited their authorship.\nScientific references are in the footer. Opinions and figures are my own, AI-generated or directly linked.\n\n\n\n\n\n\nMaterials\n\n\n\n\nslides: www.gerkovink.com/Anatomy\nsource: github.com/gerkovink/Anatomy"
  },
  {
    "objectID": "lectures/anatomy.html#terms-i-may-use",
    "href": "lectures/anatomy.html#terms-i-may-use",
    "title": "The Anatomy of an Answer",
    "section": "Terms I may use",
    "text": "Terms I may use\n\nTDGM: True data generating model\nDGP: Data generating process, closely related to the TDGM, but with all the wacky additional uncertainty\nTruth: The comparative truth that we are interested in\nBias: The distance to the comparative truth\nVariance: When not everything is the same\nEstimate: Something that we calculate or guess\nEstimand: The thing we aim to estimate and guess\nPopulation: That larger entity without sampling variance\nSample: The smaller thing with sampling variance\nIncomplete: There exists a more complete version, but we don’t have it\nObserved: What we have\nUnobserved: What we would also like to have"
  },
  {
    "objectID": "lectures/anatomy.html#at-the-start",
    "href": "lectures/anatomy.html#at-the-start",
    "title": "The Anatomy of an Answer",
    "section": "At the start",
    "text": "At the start\nLet’s start with the core:\n\n\n\n\n\n\nStatistical inference\n\n\n\nStatistical inference is the process of drawing conclusions from truths\n\n\nTruths are boring, but they are convenient.\n\nhowever, for most problems truths require a lot of calculations, tallying or a complete census.\ntherefore, a proxy of the truth is in most cases sufficient\nAn example for such a proxy is a sample\nSamples are widely used and have been for a long timeSee Jelke Bethlehem’s CBS discussion paper for an overview of the history of sampling within survey statistics\n\n\n\\(^1\\) See Jelke Bethlehem’s CBS discussion paper for an overview of the history of survey sampling"
  },
  {
    "objectID": "lectures/anatomy.html#do-we-need-data",
    "href": "lectures/anatomy.html#do-we-need-data",
    "title": "The Anatomy of an Answer",
    "section": "Do we need data?",
    "text": "Do we need data?\nWithout any data we can still come up with a statistically valid answer.\n\nThe answer will not be very informative.\nIn order for our answer to be more informative, we need more information\n\nSome sources of information can already tremendously guide the precision of our answer.\n\n\n\n\n\n\nIn Short\n\n\n\nInformation bridges the answer to the truth. Too little information may lead you to a false truth."
  },
  {
    "objectID": "lectures/anatomy.html#being-wrong-about-the-truth",
    "href": "lectures/anatomy.html#being-wrong-about-the-truth",
    "title": "The Anatomy of an Answer",
    "section": "Being wrong about the truth",
    "text": "Being wrong about the truth\n\n\n\n\n\nThe population is the truth\nThe sample comes from the population, but is generally smaller in size\nThis means that not all cases from the population can be in our sample\nIf not all information from the population is in the sample, then our sample may be wrong\n\n\n\n\n\n\n\nGood questions to ask yourself\n\n\n\n\nWhy is it important that our sample is not wrong?\nHow do we know that our sample is not wrong?"
  },
  {
    "objectID": "lectures/anatomy.html#solving-the-missingness-problem",
    "href": "lectures/anatomy.html#solving-the-missingness-problem",
    "title": "The Anatomy of an Answer",
    "section": "Solving the missingness problem",
    "text": "Solving the missingness problem\n\n\n\n\n\nThere are many flavours of sampling\nIf we give every unit in the population the same probability to be sampled, we do random sampling\nThe convenience with random sampling is that the missingness problem can be ignored\nThe missingness problem would in this case be: not every unit in the population has been observed in the sample\n\n\n\n\n\n\n\nHmmm…\n\n\n\nWould that mean that if we simply observe every potential unit, we would be unbiased about the truth?"
  },
  {
    "objectID": "lectures/anatomy.html#sidestep",
    "href": "lectures/anatomy.html#sidestep",
    "title": "The Anatomy of an Answer",
    "section": "Sidestep",
    "text": "Sidestep\n\n\n\n\n\nThe problem is a bit larger\nWe have three entities at play, here:\n\nThe truth we’re interested in\nThe proxy that we have (e.g. sample)\nThe model that we’re running\n\nThe more features we use, the more we capture about the outcome for the cases in the data\nThe more cases we have, the more we approach the true information  All these things are related to uncertainty. Our model can still yield biased results when fitted to \\(\\infty\\) features. Our inference can still be wrong when obtained on \\(\\infty\\) cases."
  },
  {
    "objectID": "lectures/anatomy.html#sidestep-1",
    "href": "lectures/anatomy.html#sidestep-1",
    "title": "The Anatomy of an Answer",
    "section": "Sidestep",
    "text": "Sidestep\n\n\n\n\n\nThe problem is a bit larger\nWe have three entities at play, here:\n\nThe truth we’re interested in\nThe proxy that we have (e.g. sample)\nThe model that we’re running\n\nThe more features we use, the more we capture about the outcome for the cases in the data\nThe more cases we have, the more we approach the true information \n\nCore assumption: all observations are bonafide"
  },
  {
    "objectID": "lectures/anatomy.html#uncertainty-simplified",
    "href": "lectures/anatomy.html#uncertainty-simplified",
    "title": "The Anatomy of an Answer",
    "section": "Uncertainty simplified",
    "text": "Uncertainty simplified\n\n\n\n\nWhen we do not have all information …\n\nWe need to accept that we are probably wrong\nWe just have to quantify how wrong we are\n\n In some cases we estimate that we are only a bit wrong. In other cases we estimate that we could be very wrong. This is the purpose of testing.  The uncertainty measures about our estimates can be used to create intervals"
  },
  {
    "objectID": "lectures/anatomy.html#confidence-in-the-answer",
    "href": "lectures/anatomy.html#confidence-in-the-answer",
    "title": "The Anatomy of an Answer",
    "section": "Confidence in the answer",
    "text": "Confidence in the answer\n\n\n\n\nAn intuitive approach to evaluating an answer is confidence. In statistics, we often use confidence intervals. Discussing confidence can be hugely informative!\nIf we sample 100 samples from a population, then a 95% CI will cover the true population value at least 95 out of 100 times.\n\nIf the coverage &lt;95: bad estimation process with risk of errors and invalid inference\nIf the coverage &gt;95: inefficient estimation process, but correct conclusions and valid inference. Lower statistical power.\n\n\n\n\nNeyman, J. (1934). On the Two Different Aspects of the Representative Method: The Method of Stratified Sampling and the Method of Purposive Selection.  Journal of the Royal Statistical Society Series A: Statistics in Society, 97(4), 558-606."
  },
  {
    "objectID": "lectures/anatomy.html#how-do-we-know-that-our-sample-is-not.",
    "href": "lectures/anatomy.html#how-do-we-know-that-our-sample-is-not.",
    "title": "The Anatomy of an Answer",
    "section": "How do we know that our sample is not….",
    "text": "How do we know that our sample is not….\n\n\n\n\nWe can replicate our sample.\n\nA replication would be a new sample from the same population or true data generating model obtained by the same data generating process.\nIf we would sample 100 times, we would get 100 different samples\nIf we would estimate 100 times, we would get 100 different estimates with 100 different confidence intervals (e.g. 95% CI)\nOut of these 100 different intervals, we would expect a nominal coverage. For a 95% CI we’d expect 95 of them to cover the true population value."
  },
  {
    "objectID": "lectures/anatomy.html#this-is-a-lot-of-work",
    "href": "lectures/anatomy.html#this-is-a-lot-of-work",
    "title": "The Anatomy of an Answer",
    "section": "This is a lot of work…",
    "text": "This is a lot of work…\n\n\n\n\nFull sampling validation of a model’s inferences is a lot of work.\n\nit is the most robust way of obtaining inferential validity\nit is not always necessary\n\nUnder some general assumptions, we can use the same data to validate our model’s inferences and predictions.\n\nthese assumptions can be met in practice\nbut as soon as assumptions are made, we open the door to errors when these assumptions do not hold"
  },
  {
    "objectID": "lectures/anatomy.html#assumptions",
    "href": "lectures/anatomy.html#assumptions",
    "title": "The Anatomy of an Answer",
    "section": "Assumptions",
    "text": "Assumptions\nTake the following definition:\n\na thing that is accepted as true or as certain to happen, without proof.\n\nAssumptions are a statisticians faith. It is often impossible to prove that they hold in practice, but we choose to believe that they do.\n\n\n\n\n\n\nSensitivity analyses\n\n\n\nI often use computational evaluation techniques to quantify the scope of the impact of assumptions made. For example, we can test the effect of violating assumptions on our results. We then verify if the inferences are sensitive to violations of the assumptions. We can even verify the extend of when assumptions start becoming influential to our inferences."
  },
  {
    "objectID": "lectures/anatomy.html#the-holy-trinity",
    "href": "lectures/anatomy.html#the-holy-trinity",
    "title": "The Anatomy of an Answer",
    "section": "The holy trinity",
    "text": "The holy trinity\nWhenever I evaluate something, I tend to look at three things:\n\nbias (how far from the truth)\nuncertainty/variance (how wide is my interval)\ncoverage (how often do I cover the truth with my interval)\n\n As a function of model complexity in specific modeling efforts, these components play a role in the bias/variance tradeoff"
  },
  {
    "objectID": "lectures/anatomy.html#on-the-individual-level",
    "href": "lectures/anatomy.html#on-the-individual-level",
    "title": "The Anatomy of an Answer",
    "section": "On the individual level",
    "text": "On the individual level\n\n\n\n\nIndividual intervals can also be hugely informative!\nIndividual intervals are generally wider than confidence intervals\n\nThis is because it covers inherent uncertainty in the data point on top of sampling uncertainty\n\n\n\n\n\n\n\nBe careful\n\n\n\nNarrower intervals mean less uncertainty.\nIt does not mean that the answer is correct!"
  },
  {
    "objectID": "lectures/anatomy.html#case-spaceshuttle-challenger",
    "href": "lectures/anatomy.html#case-spaceshuttle-challenger",
    "title": "The Anatomy of an Answer",
    "section": "Case: Spaceshuttle Challenger",
    "text": "Case: Spaceshuttle Challenger\n36 years ago, on 28 January 1986, 73 seconds into its flight and at an altitude of 9 miles, the space shuttle Challenger experienced an enormous fireball caused by one of its two booster rockets and broke up. The crew compartment continued its trajectory, reaching an altitude of 12 miles, before falling into the Atlantic. All seven crew members, consisting of five astronauts and two payload specialists, were killed."
  },
  {
    "objectID": "lectures/anatomy.html#nothing-happened-so-we-ignored-it",
    "href": "lectures/anatomy.html#nothing-happened-so-we-ignored-it",
    "title": "The Anatomy of an Answer",
    "section": "Nothing happened, so we ignored it",
    "text": "Nothing happened, so we ignored it\n\n\n\n\n\n\n\n\n\n\n\n\nIn the decision to proceed with the launch, there was a presence of dark data. And no-one noticed!\n\nDark data\n\nInformation that is not available but necessary to arrive at the correct answer.\n\n\nThis missing information has the potential to mislead people. The notion that we can be misled is essential because it also implies that artificial intelligence can be misled!\n\n\n\n\n\n\nIf you don’t have all the information, there is always the possibility of drawing an incorrect conclusion or making a wrong decision."
  },
  {
    "objectID": "lectures/anatomy.html#in-practice",
    "href": "lectures/anatomy.html#in-practice",
    "title": "The Anatomy of an Answer",
    "section": "In Practice",
    "text": "In Practice\n\n\n\n\nWe now have a new problem:\n\nwe do not have the whole truth; but merely a sample of the truth\nwe do not even have the whole sample, but merely a sample of the sample of the truth.\n\n\n\n\n\n\n\nWhat would be a simple solution to allowing for valid inferences on the incomplete sample? Would that solution work in practice?"
  },
  {
    "objectID": "lectures/anatomy.html#how-to-fix-the-missingness-problem",
    "href": "lectures/anatomy.html#how-to-fix-the-missingness-problem",
    "title": "The Anatomy of an Answer",
    "section": "How to fix the missingness problem",
    "text": "How to fix the missingness problem\n\n\n\n\nThere are two sources of uncertainty that we need to cover when analyzing incomplete data:\n\nUncertainty about the data values we don’t have:when we don’t know what the true observed value should be, we must create a distribution of values with proper variance (uncertainty).\nUncertainty about the process that generated the values we do have:nothing can guarantee that our sample is the one true sample. So it is reasonable to assume that the parameters obtained on our sample are biased.\n\nA straightforward and intuitive solution for analyzing incomplete data in such scenarios is multiple imputation (Rubin, 1987).\n\n\n\nRubin, D. B. (1987). Multiple imputation for nonresponse in surveys. John Wiley & Sons."
  },
  {
    "objectID": "lectures/anatomy.html#now-how-do-we-know-we-did-well",
    "href": "lectures/anatomy.html#now-how-do-we-know-we-did-well",
    "title": "The Anatomy of an Answer",
    "section": "Now how do we know we did well?",
    "text": "Now how do we know we did well?\n\n\n\n\n\n\nI’m really sorry!\n\n\n\nIn practice we don’t know if we did well, because we often lack the necessary comparative truths.\n\n\nFor example:\n\nPredict a future response, but we only have the past\nAnalyzing incomplete data without a reference about the truth\nEstimate the effect between two things that can never occur together\nDetecting fraudulent transactions with only access to the own transaction history\nAppealing to a new customer base with only data about existing customers\nMixing bonafide observations with bonafide non-observations"
  },
  {
    "objectID": "lectures/anatomy.html#scenario",
    "href": "lectures/anatomy.html#scenario",
    "title": "The Anatomy of an Answer",
    "section": "Scenario",
    "text": "Scenario"
  },
  {
    "objectID": "lectures/anatomy.html#scenario-1",
    "href": "lectures/anatomy.html#scenario-1",
    "title": "The Anatomy of an Answer",
    "section": "Scenario",
    "text": "Scenario\nLet’s assume that we have an incomplete data set and that we can impute (fill in) the incomplete values under multiple candidate models\nChallenge Imputing this data set under one model may yield different results than imputing this data set under another model. Identify the best model!\nProblem We have no idea about validity of either model’s results: we would need either the true observed values or the estimand before we can judge the performance and validity of the imputation model.\n\n\n\n\n\n\nNot all is lost\n\n\n\nWe do have a constant in our problem: the observed values"
  },
  {
    "objectID": "lectures/anatomy.html#solution---overimpute-the-observed-values",
    "href": "lectures/anatomy.html#solution---overimpute-the-observed-values",
    "title": "The Anatomy of an Answer",
    "section": "Solution - overimpute the observed values",
    "text": "Solution - overimpute the observed values\n\n\n\n\n\n\n\n\nCai, M., van Buuren, S., & Vink, G. (2022). Graphical and numerical diagnostic tools to assess multiple imputation models by posterior predictive checking."
  },
  {
    "objectID": "lectures/anatomy.html#scenario-2",
    "href": "lectures/anatomy.html#scenario-2",
    "title": "The Anatomy of an Answer",
    "section": "Scenario",
    "text": "Scenario\nIn a survey about research integrity and fraud we surveyed behaviours and practices in the following format.\n\n\n\n Many behaviours were surveyed over multiple groups of people. Some findings:\n\nIn most groups similar behavioural prevalence was observed.\nWhen looking at subgroups, prevalences differ between subgroups.\nNot applicables were much more prevalent in one group than in other groups\nThere are too few cases and too many patterns with Not Applicable’s over features to allow for a pattern-wise analysis (stratified analysis).\nThere are too many Not Applicables to allow for listwise deletion."
  },
  {
    "objectID": "lectures/anatomy.html#some-background",
    "href": "lectures/anatomy.html#some-background",
    "title": "The Anatomy of an Answer",
    "section": "Some background",
    "text": "Some background\nWe know:\n\nNot Applicable is not randomly distributed over the data. Removing them is therefore not valid!\nNot Applicable are bonafide missing values: there should be no observations.\n\n\n\n\n\n\n\nThere’s no such thing as a free lunch\n\n\n\nEvery imputation will bias the results. For some we know the direction of the bias, for some we have no idea. We do not have access to the truth.\n\n\n\nWhat would you do?"
  },
  {
    "objectID": "lectures/anatomy.html#our-solution",
    "href": "lectures/anatomy.html#our-solution",
    "title": "The Anatomy of an Answer",
    "section": "Our solution",
    "text": "Our solution\nWe chose to impute the data as 1 (never). There are a couple of reasons why we think that this is the best defendable scenario.\n\nNever has a semantic similarity to a behaviour not being applicable. However, Never implies intentionality; Not Applicable does not.\nWe know the effect the imputation has on the inference: Filling in Never will underestimate intentional behaviours.\n\nIn this case the choice was made to make a deliberate error. The estimates obtained would serve as an underestimation of true behaviour and can be considered a lower bound estimation."
  },
  {
    "objectID": "lectures/anatomy.html#to-conclude",
    "href": "lectures/anatomy.html#to-conclude",
    "title": "The Anatomy of an Answer",
    "section": "To conclude",
    "text": "To conclude"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Challenges",
    "section": "",
    "text": "Introductie\nWelkom bij deze cursusdag, speciaal ontworpen voor aankomende data science professionals die zich willen verdiepen in de praktische datasynthesevaardigheden die nodig zijn om incomplete dataproblemen op te lossen, datasets statistisch te matchen of digitale tweelingen te synthetiseren. Net als in de voorgaande cursusdag is het van belang om synthetiseren of imputeren en analyseren hand-in-hand te laten gaan. Deze dag staat daarom wederom in het teken van hands-on leren en samenwerken, met een sterke focus op praktische toepassing en met een statistisch valide randje.\n\n\nDoel\nHet hoofddoel van deze cursusdag is om jou, als trainee, te voorzien van de vaardigheden en kennis die nodig zijn om effectief bij te dragen aan het iteratieve dataontwikkelingsproces binnen multidisciplinaire teams. Door een combinatie van niet te lange verdiepende sessies en duidelijke praktische opdrachten, krijg je de kans om direct toe te passen wat je leert. Dit alles met het oog op het versterken van jouw vaardigheden in het ontwikkelen, testen, en analyseren van methoden op incomplete data sets. We gebruiken daarvoor vandaag het R-package mice, wat geldt als de de facto standaard in het analyseren van incomplete data.\nJe wordt uitgedaagd om je kennis van dark data en computational evaluation te verdiepen. Door middel van een reeks interactieve tutorials en workshops zul je uiteindelijk leren hoe je een eigen imputatiemodel kunt ontwikkelen, toepassen en evalueren. Het programma is zorgvuldig samengesteld om je door deze workflow te leiden, van de eerste kennismaking met simpele data imputatiemethoden tot aan complexe imputatie en data synthese.\nAan het einde van deze dag zul je een sterke basis hebben in de belangrijkste imputatie en syntheseflows en -praktijken die van belang zijn voor elke dark data scientist en zul je de onzekerheid die altijd gepaard gaat met dark data en data synthese op waarde kunnen schatten.\n\n\nVoorbereiding\nDeelnemers dienen het volgende te installeren voorafgaand aan de cursusdag:\nR en RStudio: zie https://posit.co/download/rstudio-desktop/\nInstalleer ook alvast het package mice en het package ggmice van CRAN. De package set tidyverse is ook handig.\n\n\nTeaching team\nGerko Vink & Carlos Poses"
  },
  {
    "objectID": "lectures/dark-data.html",
    "href": "lectures/dark-data.html",
    "title": "Dark Data",
    "section": "",
    "text": "Another placeholder!"
  },
  {
    "objectID": "practicals/data-synthetis-challenge.html",
    "href": "practicals/data-synthetis-challenge.html",
    "title": "Data synthesis challenge",
    "section": "",
    "text": "Introduction\nIn this challenge, you will have to synthetize data. Concretely, you have to synthetic data with one of the following two goals in mind:\n\nGetting as close as possible to the original data.\nGetting as close as possible to the inference of a model you estimate in the original data.\n\nFor synthetizing the data, we suggest you use the R package mice (van Buuren and Groothuis-Oudshoorn 2011). While mice was originally developed to impute missing data, but it can also be used to impute synthetic data (see Volker and Vink 2021). Other alternatives to create synthetic data are, for example, the R-package synthpop (Nowok, Raab, and Dibben 2016), or the stand-alone software IVEware (“IVEware: Imputation and Variance Estimation Software,” n.d.).\n\n\nData\nYou will be using the Heart failure clinical records data set, a medical data set from the UCI Machine Learning Repository. You can find more info about the data and download it here, and download it directly here. The data set contains medical information of 299 individuals on 13 variables, and is typically used to predict whether or not a patient will survive during the follow-up period, using several biomedical. We strongly recommend you create an R project, and store the data in a folder called ‘data’. predictors.\n\n\nCode\nWe will be using the following libraries. If you haven’t installed them yet, you can install them using install.packages(\"desired-library\").\n\nlibrary(mice)\nlibrary(readr)\n# add more libraries\n\nIf you have created an R project and a data folder, you will have to read the data as follows:\n\nheart_failure &lt;- read_csv(\"data/heart_failure_clinical_records_dataset.csv\")\n\nIf you haven’t created an R project, you will have to set up a working directory and refer to the the directory where the file is stored.\nThe Heart failure clinical records data consists of the following variables:\n\nage: Age in years\nanaemia: Whether the patient has a decrease of red blood cells (No/Yes)\nhypertension: Whether the patient has high blood pressure (No/Yes)\ncreatinine_phosphokinase: Level of the creatinine phosphokinase enzyme in the blood (mcg/L)\ndiabetes: Whether the patient has diabetes (No/Yes)\nejection_fraction: Percentage of blood leaving the heart at each contraction\nplatelets: Platelets in de blood (kiloplatelets/mL)\nsex: Sex (Female/Male)\nserum_creatinine: Level of serum creatinine in the blood (mg/dL)\nserum_sodium: Level of serum sodium in the blood (mg/dL)\nsmoking: Whether the patient smokes (No/Yes)\nfollow_up: Follow-up period (days)\ndeceased: Whether the patient decreased during the follow-up period\n\nAfter loading the data, it is always wise to first inspect the data, so that you have an idea what to expect.\n\nhead(heart_failure)\n\n# A tibble: 6 × 13\n    age anaemia creatinine_phosphokinase diabetes ejection_fraction\n  &lt;dbl&gt;   &lt;dbl&gt;                    &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt;\n1    75       0                      582        0                20\n2    55       0                     7861        0                38\n3    65       0                      146        0                20\n4    50       1                      111        0                20\n5    65       1                      160        1                20\n6    90       1                       47        0                40\n# ℹ 8 more variables: high_blood_pressure &lt;dbl&gt;, platelets &lt;dbl&gt;,\n#   serum_creatinine &lt;dbl&gt;, serum_sodium &lt;dbl&gt;, sex &lt;dbl&gt;, smoking &lt;dbl&gt;,\n#   time &lt;dbl&gt;, DEATH_EVENT &lt;dbl&gt;\n\n\n\nsummary(heart_failure)\n\n      age           anaemia       creatinine_phosphokinase    diabetes     \n Min.   :40.00   Min.   :0.0000   Min.   :  23.0           Min.   :0.0000  \n 1st Qu.:51.00   1st Qu.:0.0000   1st Qu.: 116.5           1st Qu.:0.0000  \n Median :60.00   Median :0.0000   Median : 250.0           Median :0.0000  \n Mean   :60.83   Mean   :0.4314   Mean   : 581.8           Mean   :0.4181  \n 3rd Qu.:70.00   3rd Qu.:1.0000   3rd Qu.: 582.0           3rd Qu.:1.0000  \n Max.   :95.00   Max.   :1.0000   Max.   :7861.0           Max.   :1.0000  \n ejection_fraction high_blood_pressure   platelets      serum_creatinine\n Min.   :14.00     Min.   :0.0000      Min.   : 25100   Min.   :0.500   \n 1st Qu.:30.00     1st Qu.:0.0000      1st Qu.:212500   1st Qu.:0.900   \n Median :38.00     Median :0.0000      Median :262000   Median :1.100   \n Mean   :38.08     Mean   :0.3512      Mean   :263358   Mean   :1.394   \n 3rd Qu.:45.00     3rd Qu.:1.0000      3rd Qu.:303500   3rd Qu.:1.400   \n Max.   :80.00     Max.   :1.0000      Max.   :850000   Max.   :9.400   \n  serum_sodium        sex            smoking            time      \n Min.   :113.0   Min.   :0.0000   Min.   :0.0000   Min.   :  4.0  \n 1st Qu.:134.0   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 73.0  \n Median :137.0   Median :1.0000   Median :0.0000   Median :115.0  \n Mean   :136.6   Mean   :0.6488   Mean   :0.3211   Mean   :130.3  \n 3rd Qu.:140.0   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:203.0  \n Max.   :148.0   Max.   :1.0000   Max.   :1.0000   Max.   :285.0  \n  DEATH_EVENT    \n Min.   :0.0000  \n 1st Qu.:0.0000  \n Median :0.0000  \n Mean   :0.3211  \n 3rd Qu.:1.0000  \n Max.   :1.0000  \n\n\nNow, you are ready to got. First, you will have to synthetize the data. Then, you will have to evaluate it with regards to a model of choice, or the original data.\n\n\nCreating synthetic data\nBroadly speaking, two methods for creating synthetic data can be distinguished. The first one is based on parametric imputation models, which assumes that the structure of the data is fixed, and draws synthetic values from a pre-specified probability distribution. That is, after estimating a statistical model, the synthetic data are generated from a probability distribution, without making any further use of the observed data. In general, this procedure is less likely to result in an accidental release of disclosive information. However, these parametric methods are often less capable of capturing the complex nature of real-world data sets.\nThe subtleties of real-world data are often better reproduced with non-parametric imputation models. Using this approach, a non-parametric model is estimated, resulting in a donor pool out of which a single observation per observation and per variable is drawn. These models thus reuse the observed data to serve as synthetic data. Accordingly, much of the values that were in the observed data end up in the synthetic data. However, these observed data are generally combined in unique ways, it is generally not possible to link this information to the original respondents. The non-parametric procedures often yield better inferences, while still being able to prevent disclosure risk (although more research into measures to qualify the remaining risks is required). We will showcase how to generate synthetic data using one such non-parametric method: classification and regression trees (CART).\nNow you have a feeling of what the data looks like, we will use these two different ways to create synthetic data: a fully parametric approach, in which the data are synthesized using either linear or logistic regression, and a fully non-parametric approach, in which we synthesize all data using CART.\n… ADD MORE TEXT HERE. I GUESS MICE EXPLANATION? …\n\n\nEvaluating Synthetic data\nOnce you have synthetized your data, you have to evaluate it. If you have chosen to evaluate it with regards to the original data, you will have to perform evaluations such as: comparing univariate distributions, multivariate distributions, and more.\nIf you have chosen to evaluate it with regards to a model, you will have to 1) estimate a model on the original data; and 2) estimate a model on the synthetic data. Then, you will have to compare the results of the two models.\nNote that, in both cases, methods are iterative: you synthetize data, evaluate it, and then go back to synthetizing data. You should do this until the results are as close as desired.\n… ADD ASSIGNMENT HERE …"
  },
  {
    "objectID": "practicals.html",
    "href": "practicals.html",
    "title": "Practicals",
    "section": "",
    "text": "Practicals\nHere you can find the links to the practicals.\n\nPractical 1: Hands-on met mice in R\nPractical 2: Data synthese challenge"
  },
  {
    "objectID": "practicals/data-synthesis-challenge.html",
    "href": "practicals/data-synthesis-challenge.html",
    "title": "Data synthesis challenge",
    "section": "",
    "text": "Introduction\nIn this challenge, you will have to synthetize data. In real life, you always have to synthetic data with one of the following two goals in mind:\n\nGetting as close as possible to the original data.\nGetting as close as possible to the inference of a model you estimate in the original data.\n\nFor synthetizing the data, we suggest you use the R package mice (van Buuren and Groothuis-Oudshoorn 2011). While mice was originally developed to impute missing data, but it can also be used to impute synthetic data (see Volker and Vink 2021). Other alternatives to create synthetic data are, for example, the R-package synthpop (Nowok, Raab, and Dibben 2016), or the stand-alone software IVEware (“IVEware: Imputation and Variance Estimation Software,” n.d.).\n\n\nData\nYou will be using the Heart failure clinical records data set, a medical data set from the UCI Machine Learning Repository. You can find more info about the data and download it here, and download it directly here. The data set contains medical information of 299 individuals on 13 variables, and is typically used to predict whether or not a patient will survive during the follow-up period, using several biomedical. We strongly recommend you create an R project, and store the data in a folder called ‘data’. predictors.\n\n\nCode\nWe will be using the following libraries. If you haven’t installed them yet, you can install them using install.packages(\"desired-library\").\n\nlibrary(mice)     # for imputation and synthesis\nlibrary(magrittr) # for pipes\nlibrary(readr)    # for reading in data\n# add more libraries\n\nYou can read in the data as follows:\n\nheart_failure &lt;- \n  url(\"https://www.gerkovink.com/challenge/practicals/data/heart_failure_clinical_records_dataset.csv\") %&gt;% read_csv()\n\nThe Heart failure clinical records data consists of the following variables:\n\nage: Age in years\nanaemia: Whether the patient has a decrease of red blood cells (No/Yes)\ncreatinine_phosphokinase: Level of the creatinine phosphokinase enzyme in the blood (mcg/L)\ndiabetes: Whether the patient has diabetes (No/Yes)\nhigh_blood_pressure: Whether the patient has high blood pressure (No/Yes)\nejection_fraction: Percentage of blood leaving the heart at each contraction\nplatelets: Platelets in de blood (kiloplatelets/mL)\nsex: Sex (Female/Male)\nserum_creatinine: Level of serum creatinine in the blood (mg/dL)\nserum_sodium: Level of serum sodium in the blood (mg/dL)\nsmoking: Whether the patient smokes (No/Yes)\nfollow_up: Follow-up period (days)\nDEATH_EVENT: Whether the patient was deceased during the follow-up period\n\nAfter loading the data, it is always wise to first inspect the data, so that you have an idea what to expect.\n\nhead(heart_failure)\n\n# A tibble: 6 × 13\n    age anaemia creatinine_phosphokinase diabetes ejection_fraction\n  &lt;dbl&gt;   &lt;dbl&gt;                    &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt;\n1    75       0                      582        0                20\n2    55       0                     7861        0                38\n3    65       0                      146        0                20\n4    50       1                      111        0                20\n5    65       1                      160        1                20\n6    90       1                       47        0                40\n# ℹ 8 more variables: high_blood_pressure &lt;dbl&gt;, platelets &lt;dbl&gt;,\n#   serum_creatinine &lt;dbl&gt;, serum_sodium &lt;dbl&gt;, sex &lt;dbl&gt;, smoking &lt;dbl&gt;,\n#   time &lt;dbl&gt;, DEATH_EVENT &lt;dbl&gt;\n\n\n\nsummary(heart_failure)\n\n      age           anaemia       creatinine_phosphokinase    diabetes     \n Min.   :40.00   Min.   :0.0000   Min.   :  23.0           Min.   :0.0000  \n 1st Qu.:51.00   1st Qu.:0.0000   1st Qu.: 116.5           1st Qu.:0.0000  \n Median :60.00   Median :0.0000   Median : 250.0           Median :0.0000  \n Mean   :60.83   Mean   :0.4314   Mean   : 581.8           Mean   :0.4181  \n 3rd Qu.:70.00   3rd Qu.:1.0000   3rd Qu.: 582.0           3rd Qu.:1.0000  \n Max.   :95.00   Max.   :1.0000   Max.   :7861.0           Max.   :1.0000  \n ejection_fraction high_blood_pressure   platelets      serum_creatinine\n Min.   :14.00     Min.   :0.0000      Min.   : 25100   Min.   :0.500   \n 1st Qu.:30.00     1st Qu.:0.0000      1st Qu.:212500   1st Qu.:0.900   \n Median :38.00     Median :0.0000      Median :262000   Median :1.100   \n Mean   :38.08     Mean   :0.3512      Mean   :263358   Mean   :1.394   \n 3rd Qu.:45.00     3rd Qu.:1.0000      3rd Qu.:303500   3rd Qu.:1.400   \n Max.   :80.00     Max.   :1.0000      Max.   :850000   Max.   :9.400   \n  serum_sodium        sex            smoking            time      \n Min.   :113.0   Min.   :0.0000   Min.   :0.0000   Min.   :  4.0  \n 1st Qu.:134.0   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 73.0  \n Median :137.0   Median :1.0000   Median :0.0000   Median :115.0  \n Mean   :136.6   Mean   :0.6488   Mean   :0.3211   Mean   :130.3  \n 3rd Qu.:140.0   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:203.0  \n Max.   :148.0   Max.   :1.0000   Max.   :1.0000   Max.   :285.0  \n  DEATH_EVENT    \n Min.   :0.0000  \n 1st Qu.:0.0000  \n Median :0.0000  \n Mean   :0.3211  \n 3rd Qu.:1.0000  \n Max.   :1.0000  \n\n\nNow, you are ready to got. First, you will have to synthetize the data. Then, you will have to evaluate it with regards to a model of choice, or the original data.\n\n\nCreating synthetic data\nBroadly speaking, two methods for creating synthetic data can be distinguished. The first one is based on parametric imputation models, which assumes that the structure of the data is fixed, and draws synthetic values from a pre-specified probability distribution. That is, after estimating a statistical model, the synthetic data are generated from a probability distribution, without making any further use of the observed data. In general, this procedure is less likely to result in an accidental release of disclosive information. However, these parametric methods are often less capable of capturing the complex nature of real-world data sets.\nThe subtleties of real-world data are often better reproduced with non-parametric imputation models. Using this approach, a non-parametric model is estimated, resulting in a donor pool out of which a single observation per observation and per variable is drawn. These models thus reuse the observed data to serve as synthetic data. Accordingly, much of the values that were in the observed data end up in the synthetic data. However, these observed data are generally combined in unique ways, it is generally not possible to link this information to the original respondents. The non-parametric procedures often yield better inferences, while still being able to prevent disclosure risk (although more research into measures to qualify the remaining risks is required). We will showcase how to generate synthetic data using one such non-parametric method: classification and regression trees (CART).\nNow you have a feeling of what the data looks like, start synthesizing the data with mice. Use this vignette for inspiration\nSome hints:\n\nuse your own seed - only unique scores are competing for the prize!\nuse the make.where and make.method functions to specify the imputation method\nuse the complete function to extract a single synthetic data from the mids object or calculate the below evaluation measures over all the imputations (extra points)\nalternatively, you can use the syn() function from the synthpop package to generate synthetic data\nplay around with the synthesis method and the number of imputations or iterations to see how the evaluation measures change\ngood luck!\n\n\n\nEvaluating Synthetic data\nOnce you have synthetized your data, you have to evaluate it. If you have chosen to evaluate it with regards to the original data, you will have to perform evaluations such as: comparing univariate distributions, multivariate distributions, and more.\nIf you have chosen to evaluate it with regards to a model, you will have to 1) estimate a model on the original data; and 2) estimate a model on the synthetic data. Then, you will have to compare the results of the two models.\nNote that, in both cases, methods are iterative: you synthetize data, evaluate it, and then go back to synthetizing data. You should do this until the results are as close as desired.\nUse the below code block to calculate the desired metrics for evaluating the synthetic data.\n\nPerformance metrics\n\n# 1. Mean absolute value of mean scaled deviation between original and synthesized variables\nsd &lt;- apply(heart_failure, 2, sd)\nMSD &lt;- colMeans(abs((syn_data-heart_failure)/sd)) |&gt; mean()\n\n# 2. Average deviation of original and synthetic correlation matrix\nMDC &lt;- mean(cor(heart_failure) - cor(syn_data))\n# 2.1 Average absolute deviation of original and synthetic correlation matrix\nMDC_abs &lt;- mean(abs(cor(heart_failure) - cor(syn_data)))\n\n# 3. Pearson divergence according to density ratio package\n# To install, use devtools::install_github(\"thomvolker/densityratio\")\nlibrary(densityratio)\n## Estimate density ratio and check summaries and plots\ndensratio &lt;- ulsif(heart_failure, syn_data) \nPD &lt;- summary(densratio)$PE\n\n# 4. Train model on synthetic data, use it to predict on observed data\n# Train model\nmodel &lt;- glm(DEATH_EVENT ~ ., data = syn_data, family = binomial)\n\n# 4.1 Compute confusion matrix of model in synthetic data\n#table(syn_data$DEATH_EVENT, predict(model, newdata = syn_data) &gt; 0.5)\n# 4.2 Compute confusion matrix of model in original data\nconf &lt;- table(heart_failure$DEATH_EVENT, predict(model, newdata = heart_failure) &gt; 0.5)\n\nlist(\"Mean Scaled Deviation: mean absolute value\" = MSD, \n               \"Correlation: mean deviation\" = MDC, \n               \"Correlation: mean absolute deviation\" = MDC_abs, \n               \"Pearson Divergence\" = PD, \n               \"Confusion Matrix\" = conf)\nplot(densratio)"
  },
  {
    "objectID": "lectures/anatomy.html",
    "href": "lectures/anatomy.html",
    "title": "The Anatomy of an Answer",
    "section": "",
    "text": "I owe a debt of gratitude to many people as the thoughts and code in these slides are the process of years-long development cycles and discussions with my team, friends, colleagues and peers. When someone has contributed to the content of the slides, I have credited their authorship.\nScientific references are in the footer. Opinions and figures are my own, AI-generated or directly linked.\n\n\n\n\n\n\nMaterials\n\n\n\n\nslides: www.gerkovink.com/Anatomy\nsource: github.com/gerkovink/Anatomy"
  },
  {
    "objectID": "lectures/anatomy/index.html#disclaimer",
    "href": "lectures/anatomy/index.html#disclaimer",
    "title": "The Anatomy of an Answer",
    "section": "Disclaimer",
    "text": "Disclaimer\nI owe a debt of gratitude to many people as the thoughts and code in these slides are the process of years-long development cycles and discussions with my team, friends, colleagues and peers. When someone has contributed to the content of the slides, I have credited their authorship.\nScientific references are in the footer. Opinions and figures are my own, AI-generated or directly linked.\n\n\n\n\n\n\nMaterials\n\n\n\nsource: github.com/gerkovink/challenge"
  },
  {
    "objectID": "lectures/anatomy/index.html#terms-i-may-use",
    "href": "lectures/anatomy/index.html#terms-i-may-use",
    "title": "The Anatomy of an Answer",
    "section": "Terms I may use",
    "text": "Terms I may use\n\nTDGM: True data generating model\nDGP: Data generating process, closely related to the TDGM, but with all the wacky additional uncertainty\nTruth: The comparative truth that we are interested in\nBias: The distance to the comparative truth\nVariance: When not everything is the same\nEstimate: Something that we calculate or guess\nEstimand: The thing we aim to estimate and guess\nPopulation: That larger entity without sampling variance\nSample: The smaller thing with sampling variance\nIncomplete: There exists a more complete version, but we don’t have it\nObserved: What we have\nUnobserved: What we would also like to have"
  },
  {
    "objectID": "lectures/anatomy/index.html#at-the-start",
    "href": "lectures/anatomy/index.html#at-the-start",
    "title": "The Anatomy of an Answer",
    "section": "At the start",
    "text": "At the start\nLet’s start with the core:\n\n\n\n\n\n\nStatistical inference\n\n\nStatistical inference is the process of drawing conclusions from truths\n\n\n\nTruths are boring, but they are convenient.\n\nhowever, for most problems truths require a lot of calculations, tallying or a complete census.\ntherefore, a proxy of the truth is in most cases sufficient\nAn example for such a proxy is a sample\nSamples are widely used and have been for a long timeSee Jelke Bethlehem’s CBS discussion paper for an overview of the history of sampling within survey statistics\n\n\n\\(^1\\) See Jelke Bethlehem’s CBS discussion paper for an overview of the history of survey sampling"
  },
  {
    "objectID": "lectures/anatomy/index.html#do-we-need-data",
    "href": "lectures/anatomy/index.html#do-we-need-data",
    "title": "The Anatomy of an Answer",
    "section": "Do we need data?",
    "text": "Do we need data?\nWithout any data we can still come up with a statistically valid answer.\n\nThe answer will not be very informative.\nIn order for our answer to be more informative, we need more information\n\nSome sources of information can already tremendously guide the precision of our answer.\n\n\n\n\n\n\nIn Short\n\n\nInformation bridges the answer to the truth. Too little information may lead you to a false truth."
  },
  {
    "objectID": "lectures/anatomy/index.html#being-wrong-about-the-truth",
    "href": "lectures/anatomy/index.html#being-wrong-about-the-truth",
    "title": "The Anatomy of an Answer",
    "section": "Being wrong about the truth",
    "text": "Being wrong about the truth\n\n\n\n\n\nThe population is the truth\nThe sample comes from the population, but is generally smaller in size\nThis means that not all cases from the population can be in our sample\nIf not all information from the population is in the sample, then our sample may be wrong\n\n\n\n\n\n\n\nGood questions to ask yourself\n\n\n\nWhy is it important that our sample is not wrong?\nHow do we know that our sample is not wrong?"
  },
  {
    "objectID": "lectures/anatomy/index.html#solving-the-missingness-problem",
    "href": "lectures/anatomy/index.html#solving-the-missingness-problem",
    "title": "The Anatomy of an Answer",
    "section": "Solving the missingness problem",
    "text": "Solving the missingness problem\n\n\n\n\n\nThere are many flavours of sampling\nIf we give every unit in the population the same probability to be sampled, we do random sampling\nThe convenience with random sampling is that the missingness problem can be ignored\nThe missingness problem would in this case be: not every unit in the population has been observed in the sample\n\n\n\n\n\n\n\nHmmm…\n\n\nWould that mean that if we simply observe every potential unit, we would be unbiased about the truth?"
  },
  {
    "objectID": "lectures/anatomy/index.html#sidestep",
    "href": "lectures/anatomy/index.html#sidestep",
    "title": "The Anatomy of an Answer",
    "section": "Sidestep",
    "text": "Sidestep\n\n\n\n\n\nThe problem is a bit larger\nWe have three entities at play, here:\n\nThe truth we’re interested in\nThe proxy that we have (e.g. sample)\nThe model that we’re running\n\nThe more features we use, the more we capture about the outcome for the cases in the data\nThe more cases we have, the more we approach the true information  All these things are related to uncertainty. Our model can still yield biased results when fitted to \\(\\infty\\) features. Our inference can still be wrong when obtained on \\(\\infty\\) cases."
  },
  {
    "objectID": "lectures/anatomy/index.html#sidestep-1",
    "href": "lectures/anatomy/index.html#sidestep-1",
    "title": "The Anatomy of an Answer",
    "section": "Sidestep",
    "text": "Sidestep\n\n\n\n\n\nThe problem is a bit larger\nWe have three entities at play, here:\n\nThe truth we’re interested in\nThe proxy that we have (e.g. sample)\nThe model that we’re running\n\nThe more features we use, the more we capture about the outcome for the cases in the data\nThe more cases we have, the more we approach the true information \n\nCore assumption: all observations are bonafide"
  },
  {
    "objectID": "lectures/anatomy/index.html#uncertainty-simplified",
    "href": "lectures/anatomy/index.html#uncertainty-simplified",
    "title": "The Anatomy of an Answer",
    "section": "Uncertainty simplified",
    "text": "Uncertainty simplified\n\n\n\n\nWhen we do not have all information …\n\nWe need to accept that we are probably wrong\nWe just have to quantify how wrong we are\n\n In some cases we estimate that we are only a bit wrong. In other cases we estimate that we could be very wrong. This is the purpose of testing.  The uncertainty measures about our estimates can be used to create intervals"
  },
  {
    "objectID": "lectures/anatomy/index.html#confidence-in-the-answer",
    "href": "lectures/anatomy/index.html#confidence-in-the-answer",
    "title": "The Anatomy of an Answer",
    "section": "Confidence in the answer",
    "text": "Confidence in the answer\n\n\n\n\nAn intuitive approach to evaluating an answer is confidence. In statistics, we often use confidence intervals. Discussing confidence can be hugely informative!\nIf we sample 100 samples from a population, then a 95% CI will cover the true population value at least 95 out of 100 times.\n\nIf the coverage &lt;95: bad estimation process with risk of errors and invalid inference\nIf the coverage &gt;95: inefficient estimation process, but correct conclusions and valid inference. Lower statistical power.\n\n\n\n\nNeyman, J. (1934). On the Two Different Aspects of the Representative Method: The Method of Stratified Sampling and the Method of Purposive Selection.  Journal of the Royal Statistical Society Series A: Statistics in Society, 97(4), 558-606."
  },
  {
    "objectID": "lectures/anatomy/index.html#how-do-we-know-that-our-sample-is-not.",
    "href": "lectures/anatomy/index.html#how-do-we-know-that-our-sample-is-not.",
    "title": "The Anatomy of an Answer",
    "section": "How do we know that our sample is not….",
    "text": "How do we know that our sample is not….\n\n\n\n\nWe can replicate our sample.\n\nA replication would be a new sample from the same population or true data generating model obtained by the same data generating process.\nIf we would sample 100 times, we would get 100 different samples\nIf we would estimate 100 times, we would get 100 different estimates with 100 different confidence intervals (e.g. 95% CI)\nOut of these 100 different intervals, we would expect a nominal coverage. For a 95% CI we’d expect 95 of them to cover the true population value."
  },
  {
    "objectID": "lectures/anatomy/index.html#this-is-a-lot-of-work",
    "href": "lectures/anatomy/index.html#this-is-a-lot-of-work",
    "title": "The Anatomy of an Answer",
    "section": "This is a lot of work…",
    "text": "This is a lot of work…\n\n\n\n\nFull sampling validation of a model’s inferences is a lot of work.\n\nit is the most robust way of obtaining inferential validity\nit is not always necessary\n\nUnder some general assumptions, we can use the same data to validate our model’s inferences and predictions.\n\nthese assumptions can be met in practice\nbut as soon as assumptions are made, we open the door to errors when these assumptions do not hold"
  },
  {
    "objectID": "lectures/anatomy/index.html#assumptions",
    "href": "lectures/anatomy/index.html#assumptions",
    "title": "The Anatomy of an Answer",
    "section": "Assumptions",
    "text": "Assumptions\nTake the following definition:\n\na thing that is accepted as true or as certain to happen, without proof.\n\nAssumptions are a statisticians faith. It is often impossible to prove that they hold in practice, but we choose to believe that they do.\n\n\n\n\n\n\nSensitivity analyses\n\n\nI often use computational evaluation techniques to quantify the scope of the impact of assumptions made. For example, we can test the effect of violating assumptions on our results. We then verify if the inferences are sensitive to violations of the assumptions. We can even verify the extend of when assumptions start becoming influential to our inferences."
  },
  {
    "objectID": "lectures/anatomy/index.html#the-holy-trinity",
    "href": "lectures/anatomy/index.html#the-holy-trinity",
    "title": "The Anatomy of an Answer",
    "section": "The holy trinity",
    "text": "The holy trinity\nWhenever I evaluate something, I tend to look at three things:\n\nbias (how far from the truth)\nuncertainty/variance (how wide is my interval)\ncoverage (how often do I cover the truth with my interval)\n\n As a function of model complexity in specific modeling efforts, these components play a role in the bias/variance tradeoff"
  },
  {
    "objectID": "lectures/anatomy/index.html#on-the-individual-level",
    "href": "lectures/anatomy/index.html#on-the-individual-level",
    "title": "The Anatomy of an Answer",
    "section": "On the individual level",
    "text": "On the individual level\n\n\n\n\nIndividual intervals can also be hugely informative!\nIndividual intervals are generally wider than confidence intervals\n\nThis is because it covers inherent uncertainty in the data point on top of sampling uncertainty\n\n\n\n\n\n\n\nBe careful\n\n\nNarrower intervals mean less uncertainty.\nIt does not mean that the answer is correct!"
  },
  {
    "objectID": "lectures/anatomy/index.html#case-spaceshuttle-challenger",
    "href": "lectures/anatomy/index.html#case-spaceshuttle-challenger",
    "title": "The Anatomy of an Answer",
    "section": "Case: Spaceshuttle Challenger",
    "text": "Case: Spaceshuttle Challenger\n36 years ago, on 28 January 1986, 73 seconds into its flight and at an altitude of 9 miles, the space shuttle Challenger experienced an enormous fireball caused by one of its two booster rockets and broke up. The crew compartment continued its trajectory, reaching an altitude of 12 miles, before falling into the Atlantic. All seven crew members, consisting of five astronauts and two payload specialists, were killed."
  },
  {
    "objectID": "lectures/anatomy/index.html#nothing-happened-so-we-ignored-it",
    "href": "lectures/anatomy/index.html#nothing-happened-so-we-ignored-it",
    "title": "The Anatomy of an Answer",
    "section": "Nothing happened, so we ignored it",
    "text": "Nothing happened, so we ignored it\n\n\n\n\n\n\n\n\n\n\n\n\nIn the decision to proceed with the launch, there was a presence of dark data. And no-one noticed!\n\nDark data\n\nInformation that is not available but necessary to arrive at the correct answer.\n\n\nThis missing information has the potential to mislead people. The notion that we can be misled is essential because it also implies that artificial intelligence can be misled!\n\n\n\n\n\n\nIf you don’t have all the information, there is always the possibility of drawing an incorrect conclusion or making a wrong decision."
  },
  {
    "objectID": "lectures/anatomy/index.html#in-practice",
    "href": "lectures/anatomy/index.html#in-practice",
    "title": "The Anatomy of an Answer",
    "section": "In Practice",
    "text": "In Practice\n\n\n\n\nWe now have a new problem:\n\nwe do not have the whole truth; but merely a sample of the truth\nwe do not even have the whole sample, but merely a sample of the sample of the truth.\n\n\n\n\n\n\n\nWhat would be a simple solution to allowing for valid inferences on the incomplete sample? Would that solution work in practice?"
  },
  {
    "objectID": "lectures/anatomy/index.html#how-to-fix-the-missingness-problem",
    "href": "lectures/anatomy/index.html#how-to-fix-the-missingness-problem",
    "title": "The Anatomy of an Answer",
    "section": "How to fix the missingness problem",
    "text": "How to fix the missingness problem\n\n\n\n\nThere are two sources of uncertainty that we need to cover when analyzing incomplete data:\n\nUncertainty about the data values we don’t have:when we don’t know what the true observed value should be, we must create a distribution of values with proper variance (uncertainty).\nUncertainty about the process that generated the values we do have:nothing can guarantee that our sample is the one true sample. So it is reasonable to assume that the parameters obtained on our sample are biased.\n\nA straightforward and intuitive solution for analyzing incomplete data in such scenarios is multiple imputation (Rubin, 1987).\n\n\n\nRubin, D. B. (1987). Multiple imputation for nonresponse in surveys. John Wiley & Sons."
  },
  {
    "objectID": "lectures/anatomy/index.html#now-how-do-we-know-we-did-well",
    "href": "lectures/anatomy/index.html#now-how-do-we-know-we-did-well",
    "title": "The Anatomy of an Answer",
    "section": "Now how do we know we did well?",
    "text": "Now how do we know we did well?\n\n\n\n\n\n\nI’m really sorry!\n\n\nIn practice we don’t know if we did well, because we often lack the necessary comparative truths.\n\n\n\nFor example:\n\nPredict a future response, but we only have the past\nAnalyzing incomplete data without a reference about the truth\nEstimate the effect between two things that can never occur together\nDetecting fraudulent transactions with only access to the own transaction history\nAppealing to a new customer base with only data about existing customers\nMixing bonafide observations with bonafide non-observations"
  },
  {
    "objectID": "lectures/anatomy/index.html#scenario",
    "href": "lectures/anatomy/index.html#scenario",
    "title": "The Anatomy of an Answer",
    "section": "Scenario",
    "text": "Scenario\nLet’s assume that we have an incomplete data set and that we can impute (fill in) the incomplete values under multiple candidate models\nChallenge Imputing this data set under one model may yield different results than imputing this data set under another model. Identify the best model!\nProblem We have no idea about validity of either model’s results: we would need either the true observed values or the estimand before we can judge the performance and validity of the imputation model.\n\n\n\n\n\n\nNot all is lost\n\n\nWe do have a constant in our problem: the observed values"
  },
  {
    "objectID": "lectures/anatomy/index.html#scenario-1",
    "href": "lectures/anatomy/index.html#scenario-1",
    "title": "The Anatomy of an Answer",
    "section": "Scenario",
    "text": "Scenario\n\n1236 citizens of Leiden who were 85 years or older on December 1, 1986 (Lagaay, Van der Meij, and Hijmans 1992).\nVisited by a physician between January 1987 and May 1989.\nA full medical history, information on current use of drugs, a venous blood sample, and other health-related data were obtained.\nBP was routinely measured during the visit.\n\nApart from some individuals who were bedridden, BP was measured while seated.\nAn Hg manometer was used and BP was rounded to the nearest 5 mmHg.\n\nThe mortality status of each individual on March 1, 1994 was retrieved from administrative sources.\n\n\nLagaay, A. M., J. C. Van der Meij, and W. Hijmans. 1992. “Validation of Medical History Taking as Part of a Population Based Survey in Subjects Aged 85 and over.” British Medical Journal 304 (6834): 1091–2."
  },
  {
    "objectID": "lectures/anatomy/index.html#solution---overimpute-the-observed-values",
    "href": "lectures/anatomy/index.html#solution---overimpute-the-observed-values",
    "title": "The Anatomy of an Answer",
    "section": "Solution - overimpute the observed values",
    "text": "Solution - overimpute the observed values\n\n\n\n\n\n\n\n\nCai, M., van Buuren, S., & Vink, G. (2022). Graphical and numerical diagnostic tools to assess multiple imputation models by posterior predictive checking."
  },
  {
    "objectID": "lectures/anatomy/index.html#scenario-2",
    "href": "lectures/anatomy/index.html#scenario-2",
    "title": "The Anatomy of an Answer",
    "section": "Scenario",
    "text": "Scenario\nIn a survey about research integrity and fraud we surveyed behaviours and practices in the following format.\n\n\n\n Many behaviours were surveyed over multiple groups of people. Some findings:\n\nIn most groups similar behavioural prevalence was observed.\nWhen looking at subgroups, prevalences differ between subgroups.\nNot applicables were much more prevalent in one group than in other groups\nThere are too few cases and too many patterns with Not Applicable’s over features to allow for a pattern-wise analysis (stratified analysis).\nThere are too many Not Applicables to allow for listwise deletion."
  },
  {
    "objectID": "lectures/anatomy/index.html#some-background",
    "href": "lectures/anatomy/index.html#some-background",
    "title": "The Anatomy of an Answer",
    "section": "Some background",
    "text": "Some background\nWe know:\n\nNot Applicable is not randomly distributed over the data. Removing them is therefore not valid!\nNot Applicable are bonafide missing values: there should be no observations.\n\n\n\n\n\n\n\nThere’s no such thing as a free lunch\n\n\nEvery imputation will bias the results. For some we know the direction of the bias, for some we have no idea. We do not have access to the truth.\n\n\n\nWhat would you do?"
  },
  {
    "objectID": "lectures/anatomy/index.html#our-solution",
    "href": "lectures/anatomy/index.html#our-solution",
    "title": "The Anatomy of an Answer",
    "section": "Our solution",
    "text": "Our solution\nWe chose to impute the data as 1 (never). There are a couple of reasons why we think that this is the best defendable scenario.\n\nNever has a semantic similarity to a behaviour not being applicable. However, Never implies intentionality; Not Applicable does not.\nWe know the effect the imputation has on the inference: Filling in Never will underestimate intentional behaviours.\n\nIn this case the choice was made to make a deliberate error. The estimates obtained would serve as an underestimation of true behaviour and can be considered a lower bound estimation."
  },
  {
    "objectID": "lectures/anatomy/index.html#to-conclude",
    "href": "lectures/anatomy/index.html#to-conclude",
    "title": "The Anatomy of an Answer",
    "section": "To conclude",
    "text": "To conclude\n\n\n\n\n\n\n\n\nGerko Vink @ UU, 31 May 2024, Utrecht"
  },
  {
    "objectID": "lectures/dark/index.html#disclaimer",
    "href": "lectures/dark/index.html#disclaimer",
    "title": "Embrace the darkness",
    "section": "Disclaimer",
    "text": "Disclaimer\nI owe a debt of gratitude to many people as the thoughts and code in these slides are the process of years-long development cycles and discussions with my team, friends, colleagues and peers. When someone has contributed to the content of the slides, I have credited their authorship.\nScientific references are in the footer. Opinions and figures are my own, AI-generated or directly linked."
  },
  {
    "objectID": "lectures/dark/index.html#clichés-out-of-the-way",
    "href": "lectures/dark/index.html#clichés-out-of-the-way",
    "title": "Embrace the darkness",
    "section": "Clichés out of the way",
    "text": "Clichés out of the way\n\nEverything is a missing data problem\n\n\nAll models are wrong, but some are useful\n\n\nHow wrong can a model be to still be useful?"
  },
  {
    "objectID": "lectures/dark/index.html#topics-for-this-lecture",
    "href": "lectures/dark/index.html#topics-for-this-lecture",
    "title": "Embrace the darkness",
    "section": "Topics for this lecture",
    "text": "Topics for this lecture\n\nProblem of dark data\nStrategies to deal with missing data\nMultiple imputation methodology to analyse incomplete data\nSynthetic data sets for disclosure protection"
  },
  {
    "objectID": "lectures/dark/index.html#why-this-focus",
    "href": "lectures/dark/index.html#why-this-focus",
    "title": "Embrace the darkness",
    "section": "Why this focus?",
    "text": "Why this focus?\n\nReal data are always incomplete\nAd-hoc fixes do not (always) work\nMultiple imputation as principled and broadly applicable approach\nGoal: get comfortable with a powerful way to deal with incomplete data and also learn to synthesize data\nWe use the mice package in R"
  },
  {
    "objectID": "lectures/dark/index.html#what-is-dark-data",
    "href": "lectures/dark/index.html#what-is-dark-data",
    "title": "Embrace the darkness",
    "section": "What is dark data?",
    "text": "What is dark data?\n\nDark data are concealed from us, and that very fact means we are at risk of misunderstanding, of drawing incorrect conclusions, and of making poor decisions.\n\n\n\n\n\nGerko Vink @ UU, 31 May 2024, Utrecht"
  },
  {
    "objectID": "lectures/dark/dark.html#disclaimer",
    "href": "lectures/dark/dark.html#disclaimer",
    "title": "Embrace the darkness",
    "section": "Disclaimer",
    "text": "Disclaimer\nI owe a debt of gratitude to many people as the thoughts and code in these slides are the process of years-long development cycles and discussions with my team, friends, colleagues and peers. When someone has contributed to the content of the slides, I have credited their authorship.\nScientific references are in the footer. Opinions and figures are my own, AI-generated or directly linked."
  },
  {
    "objectID": "lectures/dark/dark.html#clichés-out-of-the-way",
    "href": "lectures/dark/dark.html#clichés-out-of-the-way",
    "title": "Embrace the darkness",
    "section": "Clichés out of the way",
    "text": "Clichés out of the way\n\nEverything is a missing data problem\n\n\nAll models are wrong, but some are useful\n\n\nHow wrong can a model be to still be useful?"
  },
  {
    "objectID": "lectures/dark/dark.html#topics-for-this-lecture",
    "href": "lectures/dark/dark.html#topics-for-this-lecture",
    "title": "Embrace the darkness",
    "section": "Topics for this lecture",
    "text": "Topics for this lecture\n\nProblem of dark data\nStrategies to deal with missing data\nMultiple imputation methodology to analyse incomplete data\nSynthetic data sets for disclosure protection"
  },
  {
    "objectID": "lectures/dark/dark.html#what-is-dark-data",
    "href": "lectures/dark/dark.html#what-is-dark-data",
    "title": "Embrace the darkness",
    "section": "What is dark data?",
    "text": "What is dark data?\n\nDark data are concealed from us, and that very fact means we are at risk of misunderstanding, of drawing incorrect conclusions, and of making poor decisions."
  },
  {
    "objectID": "lectures/dark/dark.html#dark-data-types",
    "href": "lectures/dark/dark.html#dark-data-types",
    "title": "Embrace the darkness",
    "section": "Dark data types",
    "text": "Dark data types\n\n\n\n\n\n\n\n\n\nNo\nDescription\nNo\nDescription\n\n\n\n\n1\nData We Know Are Missing\n9\nSummaries of Data\n\n\n2\nData We Don’t Know are Missing\n10\nMeasurement Error and Uncertainty\n\n\n3\nChoosing Just Some Cases\n11\nFeedback and Gaming\n\n\n4\nSelf-Selection\n12\nInformation Asymmetry\n\n\n5\nMissing What Matters\n13\nIntentionally Darkened Data\n\n\n6\nData Which Might Have Been\n14\nFabricated and Synthetic Data\n\n\n7\nChanges with Time\n15\nExtrapolating beyond Your Data\n\n\n8\nDefinitions of Data\n16\nData not yet observable"
  },
  {
    "objectID": "lectures/dark/dark.html#concepts-definition",
    "href": "lectures/dark/dark.html#concepts-definition",
    "title": "Embrace the darkness",
    "section": "Concepts: Definition",
    "text": "Concepts: Definition\n\nMissing values are those values that are not observed\nValues do exist in theory, but we are unable to see them"
  },
  {
    "objectID": "lectures/dark/dark.html#concepts-reasons",
    "href": "lectures/dark/dark.html#concepts-reasons",
    "title": "Embrace the darkness",
    "section": "Concepts: Reasons",
    "text": "Concepts: Reasons\nMissing or dark data can occur for a lot of reasons. Or for no reason at all. For example\n\nIntentional: Sample, predict, combine, estimate\n\nrouting\nexperimental design\njoin, merge and bind operations\n\nUnintentional:\n\ndropout, refusal, concealed\ntoo far away, too small to observe\npower failure, budget exhausted, bad luck"
  },
  {
    "objectID": "lectures/dark/dark.html#consequences-why-are-missing-values-problematic",
    "href": "lectures/dark/dark.html#consequences-why-are-missing-values-problematic",
    "title": "Embrace the darkness",
    "section": "Consequences: Why are missing values problematic?",
    "text": "Consequences: Why are missing values problematic?\n\nCannot calculate, not even the mean\nLess information than planned\nEnough statistical power?\nDifferent analyses, different \\(n\\)’s\nSystematic biases in the analysis\nAppropriate confidence interval, \\(p\\)-values?\n\n\nMissing data can severely complicate interpretation and analysis"
  },
  {
    "objectID": "lectures/dark/dark.html#notation-y-r-x",
    "href": "lectures/dark/dark.html#notation-y-r-x",
    "title": "Embrace the darkness",
    "section": "Notation: \\(Y\\), \\(R\\), \\(X\\)",
    "text": "Notation: \\(Y\\), \\(R\\), \\(X\\)\n\n\n\n\n\n\n\n\n\\(Y\\) random variable with missing data\n\\(Y^\\mathrm{obs}\\) true and observed values of \\(Y\\)\n\\(Y^\\mathrm{mis}\\) true but unobserved values of \\(Y\\)\n\n\n\n\\(X\\) complete covariate\n\n\n\n\\(R\\) response indicator\n\\(R = 1\\) if \\(Y\\) is observed\n\\(R = 0\\) if \\(Y\\) is missing"
  },
  {
    "objectID": "lectures/dark/dark.html#more-notation-marginal-joint-conditional-distribution",
    "href": "lectures/dark/dark.html#more-notation-marginal-joint-conditional-distribution",
    "title": "Embrace the darkness",
    "section": "More notation: Marginal, joint & conditional distribution",
    "text": "More notation: Marginal, joint & conditional distribution\nhttps://youtu.be/JGeTcRfKgBo?t=60\n\nChapter 1 only, ends at 4:39"
  },
  {
    "objectID": "lectures/dark/dark.html#lessons-from-sam-watson-video",
    "href": "lectures/dark/dark.html#lessons-from-sam-watson-video",
    "title": "Embrace the darkness",
    "section": "Lessons from Sam Watson video",
    "text": "Lessons from Sam Watson video\n\nWhat is the difference between marginal and joint distribution?\nWhat is the difference between marginal and conditional distribution?"
  },
  {
    "objectID": "lectures/dark/dark.html#types-of-distributions",
    "href": "lectures/dark/dark.html#types-of-distributions",
    "title": "Embrace the darkness",
    "section": "Types of distributions",
    "text": "Types of distributions\n\nMarginal distribution \\(P(Y)\\)\n\nfrequency distribution/histogram of \\(Y\\)\nnormal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\)\n\nJoint distribution \\(P(Y, X)\\)\n\ncontingency table/scatterplot of \\(Y\\) and \\(X\\)\nbivariate normal distribution\n\nConditional distribution \\(P(Y | X)\\)\n\ndistribution of \\(Y\\) at a given value of \\(X\\)\nregression model with normally distributed errors"
  },
  {
    "objectID": "lectures/dark/dark.html#the-complete-data-model",
    "href": "lectures/dark/dark.html#the-complete-data-model",
    "title": "Embrace the darkness",
    "section": "The complete-data model",
    "text": "The complete-data model\n\nThe model we would like to fit if we had complete data\nThe model of scientific interest\nExamples:\n\n\\(P(Y | X, \\theta)\\): Predict blood pressure \\(Y\\) from health \\(X\\)\n\\(P(\\theta | Y)\\): Estimate gross domestic product \\(\\theta\\) from production \\(Y\\)"
  },
  {
    "objectID": "lectures/dark/dark.html#the-missing-data-model-mechanism",
    "href": "lectures/dark/dark.html#the-missing-data-model-mechanism",
    "title": "Embrace the darkness",
    "section": "The missing data model (mechanism)",
    "text": "The missing data model (mechanism)\n\nThe model that explains what is observed\nOften not of direct scientific interest\nExamples:\n\n\\(P(R | Y, X, \\psi)\\): Missingness depends on design covariates \\(X\\)\n\\(P(R | Y, \\psi)\\): Missingness depends on incomplete \\(Y\\)"
  },
  {
    "objectID": "lectures/dark/dark.html#missing-data-mechanism-a-key-assumption",
    "href": "lectures/dark/dark.html#missing-data-mechanism-a-key-assumption",
    "title": "Embrace the darkness",
    "section": "Missing data mechanism: A key assumption",
    "text": "Missing data mechanism: A key assumption\n\nWe assume we know where the missing data are\nCases where the assumption does not hold:\n\n“Tick any of the following” (we don’t know which values are real)\nTruncated data (we don’t know how many values are missing)"
  },
  {
    "objectID": "lectures/dark/dark.html#missing-data-mechanism-definition",
    "href": "lectures/dark/dark.html#missing-data-mechanism-definition",
    "title": "Embrace the darkness",
    "section": "Missing data mechanism: Definition",
    "text": "Missing data mechanism: Definition\n\nProcess that governs which \\(Y\\)s are observed and which \\(Y\\)s are unobserved (Rubin, 1976)\nSometimes we know this process (e.g.~experimental design, sampling)\nModel by response probability \\(P(R | Y^\\mathrm{obs}, Y^\\mathrm{mis}, X)\\)\nAlso called missing data model"
  },
  {
    "objectID": "lectures/dark/dark.html#mcar-missing-completely-at-random",
    "href": "lectures/dark/dark.html#mcar-missing-completely-at-random",
    "title": "Embrace the darkness",
    "section": "MCAR: Missing Completely at Random",
    "text": "MCAR: Missing Completely at Random\n\nProbability to be missing is not related to any data\n\n\n\\[\nP(R|Y^\\mathrm{obs}, Y^\\mathrm{mis}, X, \\psi) = P(R|\\psi)\n\\]\n\nExamples\n\ndata transmission error\nrandom sample\n\n\nDavid Hand calls this mechanism Not Data Dependent"
  },
  {
    "objectID": "lectures/dark/dark.html#mar-missing-at-random",
    "href": "lectures/dark/dark.html#mar-missing-at-random",
    "title": "Embrace the darkness",
    "section": "MAR: Missing at Random",
    "text": "MAR: Missing at Random\n\nProbability to be missing depends on known data\n\n\n\\[\nP(R|Y^\\mathrm{obs}, Y^\\mathrm{mis}, X, \\psi) = P(R|Y^\\mathrm{obs}, X, \\psi)\n\\]\n\nExamples\n\nIncome, where we have \\(X\\) related to wealth\nBranch patterns (e.g. how old are your children?)\n\n\nDavid Hand calls this mechanism Seen Data Dependent"
  },
  {
    "objectID": "lectures/dark/dark.html#mnar-missing-not-at-random",
    "href": "lectures/dark/dark.html#mnar-missing-not-at-random",
    "title": "Embrace the darkness",
    "section": "MNAR: Missing Not at Random",
    "text": "MNAR: Missing Not at Random\n\nProbability to be missing depends on unknown data\n\n\n\\[\nP(R|Y^\\mathrm{obs}, Y^\\mathrm{mis}, X, \\psi)\n\\]\ndoes not simplify\n\nExamples\n\nIncome, without covariates related to income\nBody weight report\n\n\nDavid Hand calls this mechanism Unseen Data Dependent"
  },
  {
    "objectID": "lectures/dark/dark.html#missing-data-mechanisms-roundup",
    "href": "lectures/dark/dark.html#missing-data-mechanisms-roundup",
    "title": "Embrace the darkness",
    "section": "Missing data mechanisms: roundup",
    "text": "Missing data mechanisms: roundup\n\nMissing Completely at Random (MCAR)\n\nmissingness is purely random\nrelatively easy to deal with\n\nMissing at Random (MAR)\n\nmissingness related to observed information\nwidely used for principled analysis\n\nMissing Not at Random (MNAR)\n\nmissingness related to unobserved information\ncannot detect this from the data\ndifficult to deal with, need context information"
  },
  {
    "objectID": "lectures/dark/dark.html#missing-data-mechanisms-graphical-representation",
    "href": "lectures/dark/dark.html#missing-data-mechanisms-graphical-representation",
    "title": "Embrace the darkness",
    "section": "Missing data mechanisms: Graphical representation",
    "text": "Missing data mechanisms: Graphical representation\n\n\nSchafer, J. L. & Graham, J. W. (2002). Missing Data. Psychological Methods, 7 (2), 147-177. doi: 10.1037/1082-989X.7.2.147"
  },
  {
    "objectID": "lectures/dark/dark.html#missing-data-mechanisms-alternative-terminology",
    "href": "lectures/dark/dark.html#missing-data-mechanisms-alternative-terminology",
    "title": "Embrace the darkness",
    "section": "Missing data mechanisms: Alternative terminology",
    "text": "Missing data mechanisms: Alternative terminology\n\nNot Data Dependent (~MCAR)\n\nIt’s missing for reasons unrelated to the data\nProbability to be missing is constant for all units\nE.g. some students not sitting an exam due to flu symptoms\n\nSeen Data Dependent (~MAR)\n\nIt’s missing for reasons related to data you have got\nProbability to be missing depends on observed data\nE.g. school discouraging lower performing students from sitting exam\n\nUnseen Data Dependent (~MNAR)\n\nMissing because of the values you would have obtained\nProbability to be missing depends on unobserved data\nE.g. students realized revised wrong material, so didn’t sit exam"
  },
  {
    "objectID": "lectures/dark/dark.html#uwe-aickelin-what-to-do-with-the-missing-data",
    "href": "lectures/dark/dark.html#uwe-aickelin-what-to-do-with-the-missing-data",
    "title": "Embrace the darkness",
    "section": "Uwe Aickelin: What to do with the missing data?",
    "text": "Uwe Aickelin: What to do with the missing data?\nhttps://www.youtube.com/watch?v=oCQbC818KKU"
  },
  {
    "objectID": "lectures/dark/dark.html#lessons-from-uwe-aickelin",
    "href": "lectures/dark/dark.html#lessons-from-uwe-aickelin",
    "title": "Embrace the darkness",
    "section": "Lessons from Uwe Aickelin",
    "text": "Lessons from Uwe Aickelin\n\nYou could obtain the data, but it’s not there\nQuality of data is going down - big data\nWhy not go back to expert? Impractical\nWhy not delete? What to delete?\nReasons for missing data are important\nMissing (Completely?) at Random\nHow impute? Mean, random, mean per group\nSoftware cannot handle missing data\nForced internet surveys"
  },
  {
    "objectID": "lectures/dark/dark.html#strategies",
    "href": "lectures/dark/dark.html#strategies",
    "title": "Embrace the darkness",
    "section": "Strategies",
    "text": "Strategies\n\nPrevention\nAd-hoc methods, e.g., single imputation, complete cases\nWeighting methods\nLikelihood methods, EM-algorithm\nMultiple imputation"
  },
  {
    "objectID": "lectures/dark/dark.html#strategies-1",
    "href": "lectures/dark/dark.html#strategies-1",
    "title": "Embrace the darkness",
    "section": "Strategies",
    "text": "Strategies\n\n\nPrevention\nAd-hoc methods, e.g., single imputation, complete cases\nWeighting methods\nLikelihood methods, EM-algorithm\nMultiple imputation"
  },
  {
    "objectID": "lectures/dark/dark.html#section",
    "href": "lectures/dark/dark.html#section",
    "title": "Embrace the darkness",
    "section": "",
    "text": "Prevent unintended missing data"
  },
  {
    "objectID": "lectures/dark/dark.html#prevention-strategies",
    "href": "lectures/dark/dark.html#prevention-strategies",
    "title": "Embrace the darkness",
    "section": "1. Prevention strategies",
    "text": "1. Prevention strategies\n\nDesign: Time intervals, Number of variables, Pilot study\nCollection: Incentives, Match interviewer-respondent, Quick follow-up, Retrieve missing data\nMeasures: Use short forms, Minimize intrusive measures, Clarity, Layout\nTreatment: Minimize burden and intensity"
  },
  {
    "objectID": "lectures/dark/dark.html#strategies-2",
    "href": "lectures/dark/dark.html#strategies-2",
    "title": "Embrace the darkness",
    "section": "Strategies",
    "text": "Strategies\n\n\nPrevention\nAd-hoc methods, e.g., single imputation, complete cases\nWeighting methods\nLikelihood methods, EM-algorithm\nMultiple imputation"
  },
  {
    "objectID": "lectures/dark/dark.html#section-1",
    "href": "lectures/dark/dark.html#section-1",
    "title": "Embrace the darkness",
    "section": "",
    "text": "Ad-hoc methods make strong assumptions"
  },
  {
    "objectID": "lectures/dark/dark.html#ad-hoc-strategies",
    "href": "lectures/dark/dark.html#ad-hoc-strategies",
    "title": "Embrace the darkness",
    "section": "2. Ad-hoc strategies",
    "text": "2. Ad-hoc strategies\n\nListwise deletion\nMean imputation\nRegression imputation\nStochastic regression imputation\nLast observation carried forward (LOCF)\nIndicator method"
  },
  {
    "objectID": "lectures/dark/dark.html#listwise-deletion",
    "href": "lectures/dark/dark.html#listwise-deletion",
    "title": "Embrace the darkness",
    "section": "Listwise deletion",
    "text": "Listwise deletion\n\nAnalyze only the complete records\nAlso know as: complete-case analysis\nAdvantages\n\nSimple (default in most software)\nUnbiased under MCAR\nConservative standard errors, significance levels\nTwo special properties in regression"
  },
  {
    "objectID": "lectures/dark/dark.html#listwise-deletion-1",
    "href": "lectures/dark/dark.html#listwise-deletion-1",
    "title": "Embrace the darkness",
    "section": "Listwise deletion",
    "text": "Listwise deletion\n\nDisadvantages\n\nWasteful\nMay not be possible\nLarger standard errors\nBiased under MAR, even for simple statistics like the mean\nInconsistencies in reporting"
  },
  {
    "objectID": "lectures/dark/dark.html#listwise-deletion-special-properties",
    "href": "lectures/dark/dark.html#listwise-deletion-special-properties",
    "title": "Embrace the darkness",
    "section": "Listwise deletion: Special properties",
    "text": "Listwise deletion: Special properties\n\nFor any regression with missing data in the predictors, estimates under listwise deletion are unbiased as long as the missingness does not depend on the outcome. Even some MNAR cases (Glynn 1986; Little 1992).\nIn logistic regression only: With missing data in either the outcome \\(Y\\) or the predictors \\(X\\) (but not both), estimates of regression weights (but not the intercept) after listwise deletion are unbiased as long as the missingness depends only on \\(Y\\) (and not on \\(X\\)!) (Vach 1994). This property is widely exploited in case-control studies in epidemiology.\nSee FIMD 2.7: https://stefvanbuuren.name/fimd/sec-when.html"
  },
  {
    "objectID": "lectures/dark/dark.html#mean-imputation",
    "href": "lectures/dark/dark.html#mean-imputation",
    "title": "Embrace the darkness",
    "section": "Mean imputation",
    "text": "Mean imputation\n\nReplace the missing values by the mean of the observed data\nAdvantages\n\nSimple\nUnbiased for the mean, under MCAR"
  },
  {
    "objectID": "lectures/dark/dark.html#mean-imputation-1",
    "href": "lectures/dark/dark.html#mean-imputation-1",
    "title": "Embrace the darkness",
    "section": "Mean imputation",
    "text": "Mean imputation"
  },
  {
    "objectID": "lectures/dark/dark.html#mean-imputation-2",
    "href": "lectures/dark/dark.html#mean-imputation-2",
    "title": "Embrace the darkness",
    "section": "Mean imputation",
    "text": "Mean imputation\n\nDisadvantages\n\nDisturbs the distribution\nUnderestimates the variance\nBiases correlations to zero\nBiased under MAR\n\nAVOID (unless you know what you are doing)"
  },
  {
    "objectID": "lectures/dark/dark.html#regression-imputation",
    "href": "lectures/dark/dark.html#regression-imputation",
    "title": "Embrace the darkness",
    "section": "Regression imputation",
    "text": "Regression imputation\n\nAlso known as prediction\n\nFit model for \\(Y^\\mathrm{obs}\\) under listwise deletion\nPredict \\(Y^\\mathrm{mis}\\) for records with missing \\(Y\\)s\nReplace missing values by prediction\n\nAdvantages\n\nUnder MAR, unbiased estimates of regression coefficients\nGood approximation to the (unknown) true data if explained variance is high\n\nFavourite among data scientists and machine learners"
  },
  {
    "objectID": "lectures/dark/dark.html#regression-imputation-1",
    "href": "lectures/dark/dark.html#regression-imputation-1",
    "title": "Embrace the darkness",
    "section": "Regression imputation",
    "text": "Regression imputation"
  },
  {
    "objectID": "lectures/dark/dark.html#regression-imputation-2",
    "href": "lectures/dark/dark.html#regression-imputation-2",
    "title": "Embrace the darkness",
    "section": "Regression imputation",
    "text": "Regression imputation\n\nDisadvantages\n\nArtificially increases correlations\nSystematically underestimates the variance\nToo optimistic \\(p\\)-values and too short confidence intervals\n\nAVOID. Harmful to statistical inference"
  },
  {
    "objectID": "lectures/dark/dark.html#stochastic-regression-imputation",
    "href": "lectures/dark/dark.html#stochastic-regression-imputation",
    "title": "Embrace the darkness",
    "section": "Stochastic regression imputation",
    "text": "Stochastic regression imputation\n\nLike regression imputation, but adds appropriate noise to the predictions to reflect uncertainty\nAdvantages\n\nPreserves the distribution of \\(Y^\\mathrm{obs}\\)\nPreserves the correlation between \\(Y\\) and \\(X\\) in the imputed data"
  },
  {
    "objectID": "lectures/dark/dark.html#stochastic-regression-imputation-1",
    "href": "lectures/dark/dark.html#stochastic-regression-imputation-1",
    "title": "Embrace the darkness",
    "section": "Stochastic regression imputation",
    "text": "Stochastic regression imputation"
  },
  {
    "objectID": "lectures/dark/dark.html#stochastic-regression-imputation-2",
    "href": "lectures/dark/dark.html#stochastic-regression-imputation-2",
    "title": "Embrace the darkness",
    "section": "Stochastic regression imputation",
    "text": "Stochastic regression imputation\n\nDisadvantages\n\nSymmetric and constant error restrictive\nSingle imputation: does not take uncertainty imputed data into account, and incorrectly treats them as real\nNot so simple anymore"
  },
  {
    "objectID": "lectures/dark/dark.html#overview-of-assumptions-needed",
    "href": "lectures/dark/dark.html#overview-of-assumptions-needed",
    "title": "Embrace the darkness",
    "section": "Overview of assumptions needed",
    "text": "Overview of assumptions needed\n\n\n\n\n\nUnbiased\n\nStandard Error\n\n\n\nMean\nReg Weight\nCorrelation\n\n\n\nListwise\nMCAR\nMCAR\nMCAR\nToo large\n\n\nPairwise\nMCAR\nMCAR\nMCAR\nComplicated\n\n\nMean\nMCAR\n–\n–\nToo small\n\n\nRegression\nMAR\nMAR\n–\nToo small\n\n\nStochastic\nMAR\nMAR\nMAR\nToo small\n\n\nLOCF\n–\n–\n–\nToo small\n\n\nIndicator\n–\n–\n–\nToo small"
  },
  {
    "objectID": "lectures/dark/dark.html#strategies-3",
    "href": "lectures/dark/dark.html#strategies-3",
    "title": "Embrace the darkness",
    "section": "Strategies",
    "text": "Strategies\n\n\nPrevention\nAd-hoc methods, e.g., single imputation, complete cases\nWeighting methods\nLikelihood methods, EM-algorithm\nMultiple imputation"
  },
  {
    "objectID": "lectures/dark/dark.html#section-2",
    "href": "lectures/dark/dark.html#section-2",
    "title": "Embrace the darkness",
    "section": "",
    "text": "Weighting minimizes bias with unit nonresponse"
  },
  {
    "objectID": "lectures/dark/dark.html#weighting",
    "href": "lectures/dark/dark.html#weighting",
    "title": "Embrace the darkness",
    "section": "3. Weighting",
    "text": "3. Weighting\n\nTake the complete cases\nRe-weight any statistic to the distribution of the covariates in the population\nAdvantages\n\nSimple (one set of weights for all incomplete variables)\nReduces bias under MAR assumption\nStandard methodology in official statistics\n\nDisadvantages\n\nDiscards data, increases the variance\nWeights may not be available\nNeeds special variance estimators\nLimited to unit non-response\n\n\nFor inferences purposes, proper imputation strategies prove to quickle become more efficient and more accurate than weighting strategies (Boeschoten et al., 2017).\n\nBoeschoten, L., Vink, G., & Hox, J. J. C. M. (2017). How to Obtain Valid Inference under Unit Nonresponse? Journal of Official Statistics, 33(4), 963-978. https://doi.org/10.1515/jos-2017-0045"
  },
  {
    "objectID": "lectures/dark/dark.html#strategies-4",
    "href": "lectures/dark/dark.html#strategies-4",
    "title": "Embrace the darkness",
    "section": "Strategies",
    "text": "Strategies\n\n\nPrevention\nAd-hoc methods, e.g., single imputation, complete cases\nWeighting methods\nLikelihood methods, EM-algorithm\nMultiple imputation"
  },
  {
    "objectID": "lectures/dark/dark.html#section-3",
    "href": "lectures/dark/dark.html#section-3",
    "title": "Embrace the darkness",
    "section": "",
    "text": "Maximum likelihood: The royal road to missing data"
  },
  {
    "objectID": "lectures/dark/dark.html#maximum-likelihood",
    "href": "lectures/dark/dark.html#maximum-likelihood",
    "title": "Embrace the darkness",
    "section": "4. Maximum likelihood",
    "text": "4. Maximum likelihood\n\nEM-algorithm, Full Information Maximum Likelihood (FIML)\nIterative methods to estimate parameters that “skip over” the missing data\nAdvantages:\n\nTheoretically sound, optimizes likelihood calculation directly\nMany applications, widely accepted\nEasy to apply (when there is software)\n\nDisadvantages:\n\nLocal minima, slow convergence\nDifficult to apply outside standard models\nComplete-data model becomes large and complex"
  },
  {
    "objectID": "lectures/dark/dark.html#strategies-5",
    "href": "lectures/dark/dark.html#strategies-5",
    "title": "Embrace the darkness",
    "section": "Strategies",
    "text": "Strategies\n\n\nPrevention\nAd-hoc methods, e.g., single imputation, complete cases\nWeighting methods\nLikelihood methods, EM-algorithm\nMultiple imputation"
  },
  {
    "objectID": "lectures/dark/dark.html#section-4",
    "href": "lectures/dark/dark.html#section-4",
    "title": "Embrace the darkness",
    "section": "",
    "text": "Multiple imputation is an all-round principled method"
  },
  {
    "objectID": "lectures/dark/dark.html#multiple-imputation",
    "href": "lectures/dark/dark.html#multiple-imputation",
    "title": "Embrace the darkness",
    "section": "5. Multiple imputation",
    "text": "5. Multiple imputation\n\nOne imputation cannot be correct in general\nImputes each missing value \\(m\\) times\nVariation between the \\(m\\) imputed values reflects our ignorance about the unknown value"
  },
  {
    "objectID": "lectures/dark/dark.html#multiple-imputation-workflow",
    "href": "lectures/dark/dark.html#multiple-imputation-workflow",
    "title": "Embrace the darkness",
    "section": "Multiple imputation workflow",
    "text": "Multiple imputation workflow"
  },
  {
    "objectID": "lectures/dark/dark.html#multiple-imputation---1987",
    "href": "lectures/dark/dark.html#multiple-imputation---1987",
    "title": "Embrace the darkness",
    "section": "Multiple imputation - 1987",
    "text": "Multiple imputation - 1987"
  },
  {
    "objectID": "lectures/dark/dark.html#acceptance-of-multiple-imputation",
    "href": "lectures/dark/dark.html#acceptance-of-multiple-imputation",
    "title": "Embrace the darkness",
    "section": "Acceptance of multiple imputation",
    "text": "Acceptance of multiple imputation\n\ncit  &lt;- c(     2022, 87, 746, NA,\n               2021, 78, 644, NA,\n               2020, 67, 575, NA,\n               2019, 62, 512, NA,\n               2018, 67, 462, NA,\n               2017, 72, 402, NA,\n               2016, 84, 370, NA,\n               2015, 62, 337, NA,\n               2014, 59, 326, NA,\n               2013, 46, 251, NA,\n               2012, 47, 230, NA,\n               2011, 55, 208, NA,\n               2010, 44, 184, NA,\n               2009, 37, 138, NA,\n               2008, 29, 116, NA,\n               2007, 34, 125, NA,\n               2006, 19, 85, NA,\n               2005, 21, 79, NA,\n               2004,  7, 58, NA,\n               2003, 18, 45, NA,\n               2002, 16, 41, NA,\n               2001, 14, 38, 57,\n               2000,  8, 21, 33,\n               1999,  6, 26, 47,\n               1998,  6, 14, 22,\n               1997,  6, 18, 29,\n               1996,  5, 13, 28,\n               1995,  3, 5, 20,\n               1994,  4, 6, 34,\n               1993,  3, 6, 15,\n               1992, NA, 4, NA,\n               1991,  3, 4, 19,\n               1990,  2, 3, 15,\n               1989, NA, 2, 11,\n               1988, NA, 1, 13,\n               1987, NA, 3, 10,\n               1986,  2, 3, 5,\n               1985, NA, NA, 1,\n               1984, NA, 1, 2,\n               1983, NA, NA, 5,\n               1982, NA, NA, 2,\n               1981, NA, NA, 1,\n               1980, NA, NA, 5,\n               1979, NA, NA, 2,\n               1978, NA, NA, 1,\n               1977, NA, NA, 2)\ncit &lt;- matrix(cit, nc=4, byrow=TRUE)\ncit &lt;- as.data.frame(cit)\nnames(cit) &lt;- c(\"Year\",\"Title\",\"Abstract\",\"All\")\npar(cex = 0.7, lwd = 0.5)\nplot(x = cit$Year, y = cit$Abstract, type=\"n\",log=\"y\",\n     xlim = c(1975,2022), ylim = c(1, 600),\n     ylab=\"Number of publications (log)\", xlab=\"Year\",\n     pch=24, bg = \"white\",\n     axes=FALSE)\nfills &lt;- viridisLite::viridis(10, alpha = 0.3, option = \"D\", direction = -1)\nrect(xleft = 1975, ybottom = 1, xright = 1996, ytop = 700, \n     col = fills[1], border = NA)\nrect(xleft = 1996, ybottom = 1, xright = 2004, ytop = 700, \n     col = fills[2], border = NA)\nrect(xleft = 2004, ybottom = 1, xright = 2012, ytop = 700, \n     col = fills[3], border = NA)\nrect(xleft = 2012, ybottom = 1, xright = 2023, ytop = 700, \n     col = fills[4], border = NA)\naxis(1, at = seq(1977, 2022, 5), lwd = par(\"lwd\"))\naxis(2, lwd = par(\"lwd\"), las=1)\nlines(x=cit$Year, y=cit$Title, pch=15, type=\"o\")\nlines(x=cit$Year, y=cit$Abstract, pch=24, type=\"o\")\nlines(x=cit$Year, y=cit$All, pch=16, type=\"o\")\nlegend(x=1975,y=200,legend=c('early publications',\n                            '\"multiple imputation\" in abstract',\n                            '\"multiple imputation\" in title'),\n       pch=c(16,2,15), bty=\"n\")\n\n\nSource: Scopus (Feb 6, 2023)"
  },
  {
    "objectID": "lectures/dark/dark.html#multiple-imputation-1",
    "href": "lectures/dark/dark.html#multiple-imputation-1",
    "title": "Embrace the darkness",
    "section": "Multiple imputation",
    "text": "Multiple imputation\n\nAdvantages\n\nCorrect point and variance estimates\nSplits missing data problem from complete-data analysis\nTheoretical properties well established\nFlexible, widely applicable\nExtensible to MNAR\n\nDisadvantages\n\nNeed to create and work with multiple imputed data sets\nMay not always be most efficient"
  },
  {
    "objectID": "lectures/dark/dark.html#conclusion",
    "href": "lectures/dark/dark.html#conclusion",
    "title": "Embrace the darkness",
    "section": "Conclusion",
    "text": "Conclusion\n\nMissing data are a fact of life, and actually interesting\nThere are many ways to treat missing data, only few are valid\nAlways try to prevent missing data\nUse ad-hoc methods with caution\nList-wise deletion up to 5% of missing data per variable\nWeighting and likelihood methods are generally valid, but may be complex\nMultiple imputation is an all-round missing-data method\n\n\n\n\n\nGerko Vink @ UU - based on slides by Stef van Buuren and Hanne Oberman and myself - 31 May 2024, Utrecht"
  },
  {
    "objectID": "lectures/dark/dark.html#what-is-the-goal-of-multiple-imputation",
    "href": "lectures/dark/dark.html#what-is-the-goal-of-multiple-imputation",
    "title": "Embrace the darkness",
    "section": "What is the goal of multiple imputation?",
    "text": "What is the goal of multiple imputation?\nThe goal:\n\nIS NOT to find the correct value for a missing data point\nIS to find an answer to the analysis problem, given that there are (many) data points missing.\n\nWe are not interested in whether the imputed value corresponds to its true counterpart in the population, but we rather sample plausible values that could have been from the posterior predictive distribution"
  },
  {
    "objectID": "lectures/dark/dark.html#demonstration-of-imputation",
    "href": "lectures/dark/dark.html#demonstration-of-imputation",
    "title": "Embrace the darkness",
    "section": "Demonstration of imputation",
    "text": "Demonstration of imputation\nLet our analysis model be\n\nboys %$% # use the exposition pipe\n  lm(hgt ~ age + tv)"
  },
  {
    "objectID": "lectures/dark/dark.html#demonstration-of-imputation-1",
    "href": "lectures/dark/dark.html#demonstration-of-imputation-1",
    "title": "Embrace the darkness",
    "section": "Demonstration of imputation",
    "text": "Demonstration of imputation\nwith output\n\nboys %$% \n  lm(hgt ~ age + tv) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = hgt ~ age + tv)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-24.679  -5.134  -0.398   5.175  23.778 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 105.4823     3.4704  30.395  &lt; 2e-16 ***\nage           3.8430     0.3262  11.782  &lt; 2e-16 ***\ntv            0.4919     0.1278   3.849 0.000155 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.389 on 221 degrees of freedom\n  (524 observations deleted due to missingness)\nMultiple R-squared:  0.7742,    Adjusted R-squared:  0.7721 \nF-statistic: 378.8 on 2 and 221 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/dark/dark.html#what-have-we-done",
    "href": "lectures/dark/dark.html#what-have-we-done",
    "title": "Embrace the darkness",
    "section": "What have we done?",
    "text": "What have we done?\nWe have used mice to obtain draws from a posterior predictive distribution of the missing data, conditional on the observed data.\nThe imputed values are mimicking the sampling variation and can be used to infer about the underlying TDGM, if and only if:\n\nThe observed data holds the information about the missing data (MAR/MCAR)"
  },
  {
    "objectID": "lectures/dark/dark.html#imputation-vs-synthetisation",
    "href": "lectures/dark/dark.html#imputation-vs-synthetisation",
    "title": "Embrace the darkness",
    "section": "Imputation vs Synthetisation",
    "text": "Imputation vs Synthetisation\nInstead of drawing only imputations from the posterior predictive distribution, we might as well overimpute the observed data."
  },
  {
    "objectID": "lectures/dark/dark.html#how-to-draw-synthetic-data-sets-with-mice",
    "href": "lectures/dark/dark.html#how-to-draw-synthetic-data-sets-with-mice",
    "title": "Embrace the darkness",
    "section": "How to draw synthetic data sets with mice",
    "text": "How to draw synthetic data sets with mice\n\nboys %&gt;% \n  mice(m = 5, method = \"cart\", printFlag = FALSE, where = matrix(TRUE, 748, 9)) %&gt;% \n  complete(\"all\") %&gt;% \n  map(~.x %$% lm(hgt ~ age + tv)) %&gt;% \n  pool() %&gt;% \n  summary()\n\n         term   estimate std.error statistic       df      p.value\n1 (Intercept) 71.4727297 0.7637067 93.586620 71.79955 8.889816e-77\n2         age  6.8882608 0.1210342 56.911699 21.81539 3.232474e-25\n3          tv -0.4038602 0.1028084 -3.928281 33.89849 3.989687e-04\n\n\n\n\n\nBut we make an error!"
  },
  {
    "objectID": "lectures/dark/dark.html#pooling-in-imputation",
    "href": "lectures/dark/dark.html#pooling-in-imputation",
    "title": "Embrace the darkness",
    "section": "Pooling in imputation",
    "text": "Pooling in imputation\nRubin (1987, p76) defined the following rules:\nFor any number of multiple imputations \\(m\\), the combination of the analysis results for any estimate \\(\\hat{Q}\\) of estimand \\(Q\\) with corresponding variance \\(U\\), can be done in terms of the average of the \\(m\\) complete-data estimates\n\\[\\bar{Q} = \\sum_{l=1}^{m}\\hat{Q}_l / m,\\]\nand the corresponding average of the \\(m\\) complete data variances\n\\[\\bar{U} = \\sum_{l=1}^{m}{U}_l / m.\\]\n\nRubin, D.B. (1987). Multiple Imputation for Nonresponse in Surveys. New York: John Wiley and Sons."
  },
  {
    "objectID": "lectures/dark/dark.html#pooling-in-imputation-1",
    "href": "lectures/dark/dark.html#pooling-in-imputation-1",
    "title": "Embrace the darkness",
    "section": "Pooling in imputation",
    "text": "Pooling in imputation\nSimply using \\(\\bar{Q}\\) and \\(\\bar{U}_m\\) to obtain our inferences would be to simplistic. In that case we would ignore any possible variation between the separate \\(\\hat{Q}_l\\) and the fact that we only generate a finite set of imputations \\(m\\). Rubin (1987, p. 76) established that the total variance \\(T\\) of \\((Q-\\bar{Q})\\) would equal\n\\[T = \\bar{U} + B + B/m,\\]\nWhere the between imputation variance \\(B\\) is defined as\n\\[B = \\sum_{l=1}^{m}(\\hat{Q}_l - \\bar{Q})^\\prime(\\hat{Q}_l - \\bar{Q}) / (m-1)\\]\nThis assumes that some of the data are observed and remain constant over the synthetic sets\nThe total variance \\(T\\) of \\((Q-\\bar{Q})\\) should (Reiter, 2003) equal\n\\[T = \\bar{U} + B/m.\\]\n\nReiter, J.P. (2003). Inference for Partially Synthetic, Public Use Microdata Sets. Survey Methodology, 29, 181-189."
  },
  {
    "objectID": "lectures/dark/dark.html#so-the-correct-code-is",
    "href": "lectures/dark/dark.html#so-the-correct-code-is",
    "title": "Embrace the darkness",
    "section": "So, the correct code is",
    "text": "So, the correct code is\n\nboys %&gt;% \n  mice(m = 5, method = \"cart\", printFlag = FALSE, where = matrix(TRUE, 748, 9)) %&gt;% \n  complete(\"all\") %&gt;% \n  map(~.x %$% lm(hgt ~ age + tv)) %&gt;% \n  pool(rule = \"reiter2003\") %&gt;% \n  summary()\n\n         term   estimate  std.error  statistic         df       p.value\n1 (Intercept) 71.3427191 0.67872263 105.113218 4670.20788  0.000000e+00\n2         age  6.9161820 0.09961066  69.432144  178.28359 5.318364e-131\n3          tv -0.4015052 0.09691269  -4.142958   56.62364  1.155999e-04"
  },
  {
    "objectID": "lectures/dark/dark.html#why-multiple-synthetic-sets",
    "href": "lectures/dark/dark.html#why-multiple-synthetic-sets",
    "title": "Embrace the darkness",
    "section": "Why multiple synthetic sets?",
    "text": "Why multiple synthetic sets?\nThank back about the goal of statistical inference: we want to go back to the true data generating model.\n\nWe do so by reverse engineering the true data generating process\nBased on our observed data\nWe do not know this process; hence multiple synthetic values\n\nThe multiplicity of the solution allows for smoothing over any Monte Carlo error that may arise from generating a single set."
  },
  {
    "objectID": "lectures/dark/dark.html#generating-more-synthetic-data",
    "href": "lectures/dark/dark.html#generating-more-synthetic-data",
    "title": "Embrace the darkness",
    "section": "Generating more synthetic data",
    "text": "Generating more synthetic data\n\nmira &lt;- boys %&gt;% \n  mice(m = 6, method = \"cart\", printFlag = FALSE, where = matrix(TRUE, 748, 9)) %&gt;% \n  list('1' = rbind(complete(., 1), complete(., 2)),\n       '2' = rbind(complete(., 3), complete(., 4)),\n       '3' = rbind(complete(., 5), complete(., 6))) %&gt;% .[-1] %&gt;% \n  data.table::setattr(\"class\", c(\"mild\", class(.))) %&gt;% \n  map(~.x %$% lm(hgt ~ reg))\n\nmira %&gt;% pool(rule = \"reiter2003\") %&gt;% \n  summary() %&gt;% tibble::column_to_rownames(\"term\") %&gt;% round(3)\n\n            estimate std.error statistic      df p.value\n(Intercept)  152.014     3.746    40.582 112.526       0\nregeast      -17.815     4.461    -3.993 771.076       0\nregwest      -23.092     4.397    -5.252  95.353       0\nregsouth     -27.579     4.451    -6.196 177.098       0\nregcity      -25.928     6.111    -4.243  19.899       0\n\nmira %&gt;% pool(rule = \"reiter2003\", \n              custom.t = \".data$ubar * 2 + .data$b / .data$m\") %&gt;% \n  summary() %&gt;% tibble::column_to_rownames(\"term\") %&gt;% round(3)\n\n            estimate std.error statistic      df p.value\n(Intercept)  152.014     5.118    29.703 112.526   0.000\nregeast      -17.815     6.228    -2.860 771.076   0.004\nregwest      -23.092     5.989    -3.856  95.353   0.000\nregsouth     -27.579     6.125    -4.503 177.098   0.000\nregcity      -25.928     7.928    -3.270  19.899   0.004\n\n\nSome adjustment to the pooling rules is neede to avoid p-inflation.\n\nRaab, Gillian M, Beata Nowok, and Chris Dibben. 2018. “Practical Data Synthesis for Large Samples”. Journal of Privacy and Confidentiality 7 (3):67-97. https://doi.org/10.29012/jpc.v7i3.407."
  },
  {
    "objectID": "lectures/dark/dark.html#some-care-is-needed",
    "href": "lectures/dark/dark.html#some-care-is-needed",
    "title": "Embrace the darkness",
    "section": "Some care is needed",
    "text": "Some care is needed\nWith synthetic data generation and synthetic data implementation come some risks.\nAny idea?"
  },
  {
    "objectID": "lectures/dark/dark.html#testing-validity",
    "href": "lectures/dark/dark.html#testing-validity",
    "title": "Embrace the darkness",
    "section": "Testing validity",
    "text": "Testing validity\nNowadays many synthetic data cowboys claim that they can generate synthetic data that looks like the real data that served as input.\nThis is like going to Madam Tusseaud’s: at face value it looks identical, but when experienced in real life it’s just not the same as the living thing.\nMany of these synthetic data packages only focus on marginal or conditional distributions. With mice we also consider the inferential properties of the synthetic data.\nIn general, we argue [^4] that any synthetic data generation procedure should\n\nPreserve marginal distributions\nPreserve conditional distribution\nYield valid inference\nYield synthetic data that are indistinguishable from the real data\n\n\nVolker, T.B.; Vink, G. Anonymiced Shareable Data: Using mice to Create and Analyze Multiply Imputed Synthetic Datasets. Psych 2021, 3, 703-716. https://doi.org/10.3390/psych3040045"
  },
  {
    "objectID": "lectures/dark/dark.html#example-from-simulation",
    "href": "lectures/dark/dark.html#example-from-simulation",
    "title": "Embrace the darkness",
    "section": "Example from simulation",
    "text": "Example from simulation\nWhen valid synthetic data are generated, the variance of the estimates is correct, such that the confidence intervals cover the population (i.e. true) value sufficiently [^5]. Take e.g. the following proportional odds model from Volker & Vink (2021):\n\n\n\n\n\n\n\n\n\nterm\nestimate\nsynthetic  bias\nsynthetic  cov\n\n\n\n\nage\n0.461\n0.002\n0.939\n\n\nhc\n-0.188\n-0.004\n0.945\n\n\nregeast\n-0.339\n0.092\n0.957\n\n\nregwest\n0.486\n-0.122\n0.944\n\n\nregsouth\n0.646\n-0.152\n0.943\n\n\nregcity\n-0.069\n0.001\n0.972\n\n\nG1\\(|\\)G2\n-6.322\n-0.254\n0.946\n\n\nG2\\(|\\)G3\n-4.501\n-0.246\n0.945\n\n\nG3\\(|\\)G4\n-3.842\n-0.244\n0.948\n\n\nG4\\(|\\)G5\n-2.639\n-0.253\n0.947\n\n\n\n\nVolker, T.B.; Vink, G. Anonymiced Shareable Data: Using mice to Create and Analyze Multiply Imputed Synthetic Datasets. Psych 2021, 3, 703-716. https://doi.org/10.3390/psych3040045"
  },
  {
    "objectID": "lectures/dark/dark.html#end-of-presentation",
    "href": "lectures/dark/dark.html#end-of-presentation",
    "title": "Embrace the darkness",
    "section": "End of presentation",
    "text": "End of presentation\n\n\n\n\nA. Bacall\n\n\n\n\n\n\n\nGerko Vink @ UU - based on slides by Stef van Buuren, Hanne Oberman and myself - 31 May 2024, Utrecht"
  },
  {
    "objectID": "lectures/dark/dark.html#demonstration-of-imputation-2",
    "href": "lectures/dark/dark.html#demonstration-of-imputation-2",
    "title": "Embrace the darkness",
    "section": "Demonstration of imputation",
    "text": "Demonstration of imputation\ngenerated on 224 cases. The full data size is\n\nboys %&gt;% dim()\n\n[1] 748   9"
  },
  {
    "objectID": "lectures/dark/dark.html#demonstration-of-imputation-3",
    "href": "lectures/dark/dark.html#demonstration-of-imputation-3",
    "title": "Embrace the darkness",
    "section": "Demonstration of imputation",
    "text": "Demonstration of imputation\nTo impute and analyze the same model with mice, we can simply run:\n\nboys %&gt;% \n  mice(m = 5, method = \"cart\", printFlag = FALSE) %&gt;% \n  complete(\"all\") %&gt;% \n  map(~.x %$% lm(hgt ~ age + tv)) %&gt;% \n  pool() %&gt;% \n  summary()\n\n         term   estimate  std.error  statistic       df      p.value\n1 (Intercept) 71.5467019 0.61617119 116.114975 735.4492 0.000000e+00\n2         age  7.0475726 0.09475359  74.377898  75.9456 1.043045e-72\n3          tv -0.5577935 0.09163996  -6.086793  39.9114 3.598196e-07"
  },
  {
    "objectID": "lectures/anatomy/index.html#problem",
    "href": "lectures/anatomy/index.html#problem",
    "title": "The Anatomy of an Answer",
    "section": "Problem",
    "text": "Problem\n\nBP was measured less frequently for very old persons and for persons with health problems.\nBP was measured more often if the BP was too high, for example if the respondent indicated a previous diagnosis of hypertension, or if the respondent used any medication against hypertension.\nThe missing data rate of BP also varied during the period of data collection.\n\nThe rate gradually increases during the first seven months of the sampling period from 5 to 40 percent of the cases, and then suddenly drops to a fairly constant level of 10–15 percent.\nA complicating factor here is that the sequence in which the respondents were interviewed was not random.\nHigh-risk groups, that is, elderly in hospitals and nursing homes and those over 95, were visited first."
  },
  {
    "objectID": "lectures/anatomy/index.html#survival-rate",
    "href": "lectures/anatomy/index.html#survival-rate",
    "title": "The Anatomy of an Answer",
    "section": "Survival rate",
    "text": "Survival rate\n\n\nVan Buuren, S. (2018). Flexible imputation of missing data. CRC press. Chapter 9.2"
  },
  {
    "objectID": "lectures/anatomy/index.html#sensitivity-analysis",
    "href": "lectures/anatomy/index.html#sensitivity-analysis",
    "title": "The Anatomy of an Answer",
    "section": "Sensitivity analysis",
    "text": "Sensitivity analysis\n\n\nVan Buuren, S. (2018). Flexible imputation of missing data. CRC press. Chapter 9.2"
  },
  {
    "objectID": "lectures/anatomy/index.html#missingness",
    "href": "lectures/anatomy/index.html#missingness",
    "title": "The Anatomy of an Answer",
    "section": "Missingness",
    "text": "Missingness\n\n\nVan Buuren, S. (2018). Flexible imputation of missing data. CRC press. Chapter 9.2"
  }
]