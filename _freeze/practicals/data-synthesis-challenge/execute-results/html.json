{
  "hash": "d6405dbd784edd2077d3d9360ce6ddeb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Data synthesis challenge\"\neditor: source\n---\n\n\n\n# **Introduction**\n\nIn this challenge, you will have to synthesize data. In real life, you always have to synthetic data with *one* of the following two goals in mind:\n\n1. Getting as close as possible to the original data.\n\n2. Getting as close as possible to the inference of a model you estimate in the original data.\n\nFor synthetizing (or is it synthesize?) the data, we suggest you use the `R` package `mice` ([van Buuren and Groothuis-Oudshoorn 2011](https://www.gerkovink.com/syn/UMCUSynthetic.html#ref-mice)). While `mice` was originally developed to impute missing data, but it can also be used to impute synthetic data (see [Volker and Vink 2021](https://www.gerkovink.com/syn/UMCUSynthetic.html#ref-volker_vink_synthetic_mice_2021)). Other alternatives to create synthetic data are, for example, the R-package `synthpop` ([Nowok, Raab, and Dibben 2016](https://www.gerkovink.com/syn/UMCUSynthetic.html#ref-synthpop)), or the stand-alone software `IVEware` ([“IVEware: Imputation and Variance Estimation Software,” n.d.](https://www.gerkovink.com/syn/UMCUSynthetic.html#ref-iveware)).\n\n\n# **Data**\n\nYou will be using the Heart failure clinical records data set, a medical data set from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/). You can find more info about the data and download it [here](https://archive.ics.uci.edu/dataset/519/heart+failure+clinical+records), and download it directly [here](data/heart_failure_clinical_records_dataset.csv). The data set contains medical information of 299 individuals on 13 variables, and is typically used to predict whether or not a patient will survive during the follow-up period, using several biomedical. We strongly recommend you create an R project, and store the data in a folder called 'data'. predictors.\n\n# **Code**\n\nWe will be using the following libraries. If you haven't installed them yet, you can install them using `install.packages(\"desired-library\")`.  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mice)     # for imputation and synthesis\nlibrary(magrittr) # for pipes\nlibrary(readr)    # for reading in data\n# add more libraries\n```\n:::\n\n\n\nYou can read in the data as follows:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nheart_failure <- \n  url(\"https://www.gerkovink.com/challenge/practicals/data/heart_failure_clinical_records_dataset.csv\") %>% read_csv()\n```\n:::\n\n\n\nThe Heart failure clinical records data consists of the following variables:\n\n- `age`: Age in years\n- `anaemia`: Whether the patient has a decrease of red blood cells (No/Yes)\n- `creatinine_phosphokinase`: Level of the creatinine phosphokinase enzyme in the blood (mcg/L)\n- `diabetes`: Whether the patient has diabetes (No/Yes)\n- `high_blood_pressure`: Whether the patient has high blood pressure (No/Yes)\n- `ejection_fraction`: Percentage of blood leaving the heart at each contraction\n- `platelets`: Platelets in de blood (kiloplatelets/mL)\n- `sex`: Sex (Female/Male)\n- `serum_creatinine`: Level of serum creatinine in the blood (mg/dL)\n- `serum_sodium`: Level of serum sodium in the blood (mg/dL)\n- `smoking`: Whether the patient smokes (No/Yes)\n- `follow_up`: Follow-up period (days)\n- `DEATH_EVENT`: Whether the patient was deceased during the follow-up period\n\nAfter loading the data, it is always wise to first inspect the data, so that you have an idea what to expect.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(heart_failure)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 13\n    age anaemia creatinine_phosphokinase diabetes ejection_fraction\n  <dbl>   <dbl>                    <dbl>    <dbl>             <dbl>\n1    75       0                      582        0                20\n2    55       0                     7861        0                38\n3    65       0                      146        0                20\n4    50       1                      111        0                20\n5    65       1                      160        1                20\n6    90       1                       47        0                40\n# ℹ 8 more variables: high_blood_pressure <dbl>, platelets <dbl>,\n#   serum_creatinine <dbl>, serum_sodium <dbl>, sex <dbl>, smoking <dbl>,\n#   time <dbl>, DEATH_EVENT <dbl>\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(heart_failure)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      age           anaemia       creatinine_phosphokinase    diabetes     \n Min.   :40.00   Min.   :0.0000   Min.   :  23.0           Min.   :0.0000  \n 1st Qu.:51.00   1st Qu.:0.0000   1st Qu.: 116.5           1st Qu.:0.0000  \n Median :60.00   Median :0.0000   Median : 250.0           Median :0.0000  \n Mean   :60.83   Mean   :0.4314   Mean   : 581.8           Mean   :0.4181  \n 3rd Qu.:70.00   3rd Qu.:1.0000   3rd Qu.: 582.0           3rd Qu.:1.0000  \n Max.   :95.00   Max.   :1.0000   Max.   :7861.0           Max.   :1.0000  \n ejection_fraction high_blood_pressure   platelets      serum_creatinine\n Min.   :14.00     Min.   :0.0000      Min.   : 25100   Min.   :0.500   \n 1st Qu.:30.00     1st Qu.:0.0000      1st Qu.:212500   1st Qu.:0.900   \n Median :38.00     Median :0.0000      Median :262000   Median :1.100   \n Mean   :38.08     Mean   :0.3512      Mean   :263358   Mean   :1.394   \n 3rd Qu.:45.00     3rd Qu.:1.0000      3rd Qu.:303500   3rd Qu.:1.400   \n Max.   :80.00     Max.   :1.0000      Max.   :850000   Max.   :9.400   \n  serum_sodium        sex            smoking            time      \n Min.   :113.0   Min.   :0.0000   Min.   :0.0000   Min.   :  4.0  \n 1st Qu.:134.0   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 73.0  \n Median :137.0   Median :1.0000   Median :0.0000   Median :115.0  \n Mean   :136.6   Mean   :0.6488   Mean   :0.3211   Mean   :130.3  \n 3rd Qu.:140.0   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:203.0  \n Max.   :148.0   Max.   :1.0000   Max.   :1.0000   Max.   :285.0  \n  DEATH_EVENT    \n Min.   :0.0000  \n 1st Qu.:0.0000  \n Median :0.0000  \n Mean   :0.3211  \n 3rd Qu.:1.0000  \n Max.   :1.0000  \n```\n\n\n:::\n:::\n\n\n\nNow, you are ready to got. First, you will have to synthetize the data. Then, you will have to evaluate it with regards to a model of choice, or the original data.\n\n\n# **Creating synthetic data**\n\nBroadly speaking, two methods for creating synthetic data can be distinguished. The first one is based on parametric imputation models, which assumes that the structure of the data is fixed, and draws synthetic values from a pre-specified probability distribution. That is, after estimating a statistical model, the synthetic data are generated from a probability distribution, without making any further use of the observed data. In general, this procedure is less likely to result in an accidental release of disclosive information. However, these parametric methods are often less capable of capturing the complex nature of real-world data sets.\n\nThe subtleties of real-world data are often better reproduced with non-parametric imputation models. Using this approach, a non-parametric model is estimated, resulting in a donor pool out of which a single observation per observation and per variable is drawn. These models thus reuse the observed data to serve as synthetic data. Accordingly, much of the values that were in the observed data end up in the synthetic data. However, these observed data are generally combined in unique ways, it is generally not possible to link this information to the original respondents. The non-parametric procedures often yield better inferences, while still being able to prevent disclosure risk (although more research into measures to qualify the remaining risks is required). We will showcase how to generate synthetic data using one such non-parametric method: classification and regression trees (CART).\n\nNow you have a feeling of what the data looks like, start synthesizing the data with `mice`. Use [this vignette for inspiration](https://www.gerkovink.com/miceVignettes/synthetic/synthetic.html)\n\nSome hints:\n\n- use your own seed - only unique scores are competing for the prize!\n- use the `make.where` and `make.method` functions to specify the imputation method\n- use the `complete` function to extract a single synthetic data from the `mids` object or calculate the below evaluation measures over all the imputations (extra points)\n- alternatively, you can use the `syn()` function from the `synthpop` package to generate synthetic data\n- play around with the synthesis method and the number of imputations or iterations to see how the evaluation measures change\n- good luck!\n\n# **Evaluating Synthetic data**\n\nOnce you have synthetized your data, you have to evaluate it. If you have chosen to evaluate it with regards to the original data, you will have to perform evaluations such as: comparing univariate distributions, multivariate distributions, and more.\n\nIf you have chosen to evaluate it with regards to a model, you will have to 1) estimate a model on the original data; and 2) estimate a model on the synthetic data. Then, you will have to compare the results of the two models.\n\nNote that, in both cases, methods are iterative: you synthetize data, evaluate it, and then go back to synthetizing data. You should do this until the results are as close as desired.\n\nUse the below code block to calculate the desired metrics for evaluating the synthetic data. \n\n\n\n::: {.cell}\n\n:::\n\n\n\n### Performance metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Mean absolute value of mean scaled deviation between original and synthesized variables\nsd <- apply(heart_failure, 2, sd)\nMSD <- colMeans(abs((syn_data-heart_failure)/sd)) |> mean()\n\n# 2. Average deviation of original and synthetic correlation matrix\nMDC <- mean(cor(heart_failure) - cor(syn_data))\n# 2.1 Average absolute deviation of original and synthetic correlation matrix\nMDC_abs <- mean(abs(cor(heart_failure) - cor(syn_data)))\n\n# 3. Pearson divergence according to density ratio package\n# To install, use \n# install.packages('densityratio', repos = 'https://thomvolker.r-universe.dev')\nlibrary(densityratio)\n## Estimate density ratio and check summaries and plots\ndensratio <- ulsif(heart_failure, syn_data) \nPD <- summary(densratio)$PE\n\n# 4. Train model on synthetic data, use it to predict on observed data\n# Train model\nmodel <- glm(DEATH_EVENT ~ ., data = syn_data, family = binomial)\n\n# 4.1 Compute confusion matrix of model in synthetic data\n#table(syn_data$DEATH_EVENT, predict(model, newdata = syn_data) > 0.5)\n# 4.2 Compute confusion matrix of model in original data\nconf <- table(heart_failure$DEATH_EVENT, predict(model, newdata = heart_failure) > 0.5)\n\nlist(\"Mean Scaled Deviation: mean absolute value\" = MSD, \n               \"Correlation: mean deviation\" = MDC, \n               \"Correlation: mean absolute deviation\" = MDC_abs, \n               \"Pearson Divergence\" = PD, \n               \"Confusion Matrix\" = conf)\nplot(densratio)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}